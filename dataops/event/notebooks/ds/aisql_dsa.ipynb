{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c7182",
   "metadata": {
    "language": "python",
    "name": "check_snowflake_data"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "st.markdown(\"## üîç Check What's Actually in Snowflake Table\")\n",
    "\n",
    "# Query to check the actual data in Snowflake\n",
    "check_sql = \"\"\"\n",
    "SELECT \n",
    "    primary_ticker,\n",
    "    event_timestamp,\n",
    "    LENGTH(transcript::STRING) AS transcript_json_length,\n",
    "    transcript\n",
    "FROM unique_transcripts\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "if st.button(\"üîç Check Snowflake Data\", use_container_width=True):\n",
    "    with st.spinner(\"Querying Snowflake table...\"):\n",
    "        try:\n",
    "            result = session.sql(check_sql).collect()\n",
    "            \n",
    "            if result:\n",
    "                row = result[0]\n",
    "                ticker = row['PRIMARY_TICKER']\n",
    "                json_length = row['TRANSCRIPT_JSON_LENGTH']\n",
    "                transcript_json = row['TRANSCRIPT']\n",
    "                \n",
    "                st.success(f\"‚úÖ Found data for {ticker}\")\n",
    "                st.metric(\"JSON Length in Snowflake\", f\"{json_length:,} characters\")\n",
    "                \n",
    "                # Try to parse the JSON\n",
    "                import json as py_json\n",
    "                try:\n",
    "                    if isinstance(transcript_json, str):\n",
    "                        parsed = py_json.loads(transcript_json)\n",
    "                    else:\n",
    "                        parsed = transcript_json\n",
    "                    \n",
    "                    num_entries = len(parsed.get('parsed_transcript', []))\n",
    "                    num_speakers = len(parsed.get('speaker_mapping', []))\n",
    "                    \n",
    "                    st.success(f\"‚úÖ JSON is valid\")\n",
    "                    col1, col2 = st.columns(2)\n",
    "                    col1.metric(\"Transcript Entries\", num_entries)\n",
    "                    col2.metric(\"Speakers\", num_speakers)\n",
    "                    \n",
    "                    # Show speaker breakdown\n",
    "                    st.subheader(\"Speaker Breakdown:\")\n",
    "                    for speaker in parsed.get('speaker_mapping', []):\n",
    "                        name = speaker['speaker_data'].get('name', 'Unknown')\n",
    "                        role = speaker['speaker_data'].get('role', 'N/A')\n",
    "                        st.write(f\"- {name} ({role})\")\n",
    "                    \n",
    "                    # Show first 3 entries\n",
    "                    st.subheader(\"First 3 Transcript Entries:\")\n",
    "                    for i, entry in enumerate(parsed.get('parsed_transcript', [])[:3]):\n",
    "                        speaker_id = entry['speaker']\n",
    "                        text = entry['text'][:200] + \"...\" if len(entry['text']) > 200 else entry['text']\n",
    "                        st.text_area(f\"Entry {i+1} (Speaker {speaker_id})\", value=text, height=100)\n",
    "                    \n",
    "                    # Show last 3 entries to check if Q&A is there\n",
    "                    st.subheader(\"Last 3 Transcript Entries (should be Q&A):\")\n",
    "                    for i, entry in enumerate(parsed.get('parsed_transcript', [])[-3:]):\n",
    "                        speaker_id = entry['speaker']\n",
    "                        text = entry['text'][:200] + \"...\" if len(entry['text']) > 200 else entry['text']\n",
    "                        # Find speaker name\n",
    "                        speaker_info = next((s for s in parsed.get('speaker_mapping', []) if s['speaker'] == speaker_id), None)\n",
    "                        speaker_name = speaker_info['speaker_data'].get('name', 'Unknown') if speaker_info else 'Unknown'\n",
    "                        speaker_role = speaker_info['speaker_data'].get('role', '') if speaker_info else ''\n",
    "                        \n",
    "                        st.text_area(f\"{speaker_name} ({speaker_role})\", value=text, height=100)\n",
    "                    \n",
    "                    # Compare with expected CSV length\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"Comparison with CSV:\")\n",
    "                    st.write(\"**Expected from CSV**: ~16,629 characters with 31 entries\")\n",
    "                    st.write(f\"**Actual in Snowflake**: {json_length:,} characters with {num_entries} entries\")\n",
    "                    \n",
    "                    if num_entries < 31:\n",
    "                        st.error(f\"‚ö†Ô∏è DATA TRUNCATED! Missing {31 - num_entries} entries\")\n",
    "                    elif json_length < 15000:\n",
    "                        st.error(f\"‚ö†Ô∏è JSON appears truncated! Expected ~16,629 chars\")\n",
    "                    else:\n",
    "                        st.success(\"‚úÖ Data appears complete!\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Failed to parse JSON: {str(e)}\")\n",
    "                    st.text_area(\"Raw transcript (first 2000 chars):\", \n",
    "                               value=str(transcript_json)[:2000],\n",
    "                               height=300)\n",
    "            else:\n",
    "                st.warning(\"No data found in table\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error querying table: {str(e)}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "else:\n",
    "    st.info(\"üëÜ Click to check what data is actually in the Snowflake table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d4e4c",
   "metadata": {
    "language": "python",
    "name": "reload_enhanced_transcripts"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "st.markdown(\"## üîÑ Reload Enhanced Transcripts\")\n",
    "st.markdown(\"\"\"\n",
    "The transcript data has been enhanced to include analyst Q&A sections for all companies.\n",
    "Click the button below to reload the `unique_transcripts` table with the enhanced data.\n",
    "\"\"\")\n",
    "\n",
    "if st.button(\"üîÑ Reload Enhanced Transcripts\", use_container_width=True):\n",
    "    with st.spinner(\"Reloading enhanced transcript data...\"):\n",
    "        try:\n",
    "            # Upload file to stage\n",
    "            st.info(\"Step 1: Uploading enhanced CSV to stage...\")\n",
    "            session.sql(\"\"\"\n",
    "                PUT file:///Users/boconnor/fsi-cortex-assistant/dataops/event/DATA/unique_transcripts.csv \n",
    "                @CSV_DATA_STAGE \n",
    "                auto_compress=false \n",
    "                overwrite=true\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            # Backup existing data\n",
    "            st.info(\"Step 2: Creating backup...\")\n",
    "            session.sql(\"\"\"\n",
    "                CREATE OR REPLACE TABLE unique_transcripts_backup AS \n",
    "                SELECT * FROM unique_transcripts\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            # Truncate table\n",
    "            st.info(\"Step 3: Truncating table...\")\n",
    "            session.sql(\"TRUNCATE TABLE unique_transcripts\").collect()\n",
    "            \n",
    "            # Reload data\n",
    "            st.info(\"Step 4: Loading enhanced data...\")\n",
    "            result = session.sql(\"\"\"\n",
    "                COPY INTO unique_transcripts\n",
    "                FROM @CSV_DATA_STAGE/unique_transcripts.csv\n",
    "                FILE_FORMAT = (FORMAT_NAME = 'CSV_FORMAT')\n",
    "                ON_ERROR = 'CONTINUE'\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            rows_loaded = result[0]['rows_loaded'] if result else 0\n",
    "            \n",
    "            # Verify\n",
    "            st.info(\"Step 5: Verifying data...\")\n",
    "            verify_df = session.sql(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_transcripts,\n",
    "                    COUNT(DISTINCT primary_ticker) as unique_tickers\n",
    "                FROM unique_transcripts\n",
    "            \"\"\").to_pandas()\n",
    "            \n",
    "            st.success(f\"‚úÖ Successfully loaded {rows_loaded} rows!\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            col1.metric(\"Total Transcripts\", verify_df['TOTAL_TRANSCRIPTS'].iloc[0])\n",
    "            col2.metric(\"Unique Tickers\", verify_df['UNIQUE_TICKERS'].iloc[0])\n",
    "            \n",
    "            # Check analyst coverage\n",
    "            st.subheader(\"Analyst Coverage Check\")\n",
    "            analyst_check = session.sql(\"\"\"\n",
    "                WITH analyst_counts AS (\n",
    "                    SELECT \n",
    "                        primary_ticker,\n",
    "                        ARRAY_SIZE(PARSE_JSON(transcript):parsed_transcript) as total_entries,\n",
    "                        ARRAY_SIZE(ARRAY_AGG(s.value) \n",
    "                            FILTER (WHERE s.value:speaker_data.role = 'Analyst')) as analyst_count\n",
    "                    FROM unique_transcripts,\n",
    "                    LATERAL FLATTEN(input => PARSE_JSON(transcript):speaker_mapping) s\n",
    "                    GROUP BY primary_ticker, transcript\n",
    "                )\n",
    "                SELECT \n",
    "                    COUNT(*) as transcripts_with_analysts,\n",
    "                    MIN(analyst_count) as min_analysts,\n",
    "                    MAX(analyst_count) as max_analysts,\n",
    "                    ROUND(AVG(analyst_count), 1) as avg_analysts\n",
    "                FROM analyst_counts\n",
    "                WHERE analyst_count > 0\n",
    "            \"\"\").to_pandas()\n",
    "            \n",
    "            col1, col2, col3, col4 = st.columns(4)\n",
    "            col1.metric(\"Transcripts with Analysts\", analyst_check['TRANSCRIPTS_WITH_ANALYSTS'].iloc[0])\n",
    "            col2.metric(\"Min Analysts\", analyst_check['MIN_ANALYSTS'].iloc[0])\n",
    "            col3.metric(\"Max Analysts\", analyst_check['MAX_ANALYSTS'].iloc[0])\n",
    "            col4.metric(\"Avg Analysts\", analyst_check['AVG_ANALYSTS'].iloc[0])\n",
    "            \n",
    "            if analyst_check['TRANSCRIPTS_WITH_ANALYSTS'].iloc[0] == 92:\n",
    "                st.success(\"‚úÖ All 92 transcripts now have analyst Q&A sections!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error reloading data: {str(e)}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "else:\n",
    "    st.info(\"üëÜ Click the button above to reload the enhanced transcript data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f89fa7-3d1d-4c88-be95-888f326dfcb1",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Cortex for Financial Services\n",
    "Snowflake has been the trusted data platform for our financial services customers, and we are excited to show case all our latest AI functionality to further supercharge your workflow. In this notebook, we will walk through:\n",
    "\n",
    "1. Leverage Snowflake marketplace to access market data\n",
    "2. Leverage Snowflake Cortex AISQL to process the unstructured data\n",
    "3. Leverage Snowflake Data Science Agent to accelerate ML model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d11480-6697-4e78-ac6f-531b5662532f",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "earnings_call_example"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# --- Initialize Session and Ticker List ---\n",
    "# Get the active Snowpark session\n",
    "session = get_active_session()\n",
    "\n",
    "# This holds the list of tickers. It fetches a live list from Snowflake on the first run.\n",
    "if 'tickers_list' not in st.session_state:\n",
    "    with st.spinner(\"Fetching initial ticker list from Snowflake...\"):\n",
    "        ticker_df = session.table('ai_transcripts_analysts_sentiments').select('PRIMARY_TICKER').distinct().limit(10)\n",
    "        st.session_state.tickers_list = [row['PRIMARY_TICKER'] for row in ticker_df.collect()]\n",
    "\n",
    "# --- Custom CSS for styling ---\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "        /* Background is now white (Streamlit default) */\n",
    "        .stTextArea, .stCodeBlock, .stDataFrame {\n",
    "            background-color: #F0F2F6; /* Light gray for contrast on white */\n",
    "            border: 1px solid #D1D9E1; /* Subtle border */\n",
    "        }\n",
    "        h1, h2, h3 {\n",
    "            color: #001f3f !important; /* Navy Blue */\n",
    "        }\n",
    "        .block-container {\n",
    "            padding-top: 2rem;\n",
    "        }\n",
    "        .stButton>button {\n",
    "            border-color: #29B5E8; /* Snowflake blue */\n",
    "            background-color: #29B5E8;\n",
    "            color: white;\n",
    "            margin-bottom: 2rem; /* Add space after the button */\n",
    "        }\n",
    "        .stButton>button:hover {\n",
    "            border-color: #1F8CBF; /* Darker blue */\n",
    "            background-color: #1F8CBF;\n",
    "        }\n",
    "        /* Style for the metric value */\n",
    "        [data-testid=\"stMetricValue\"] {\n",
    "            font-size: 2.5rem;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- AISQL Query Definition ---\n",
    "aisql_query = \"\"\"-- Create analyst sentiment table from raw transcripts\n",
    "CREATE OR REPLACE TABLE ai_transcripts_analysts_sentiments AS (\n",
    "    WITH\n",
    "    ai_analysis AS (\n",
    "        SELECT\n",
    "            primary_ticker,\n",
    "            event_timestamp,\n",
    "            transcript,\n",
    "            ai_complete(\n",
    "                'claude-4-sonnet',\n",
    "                CONCAT_WS('\\\\n',\n",
    "                    'You are analyzing sell side and buy side analysts sentiment in a public company''s earnings call transcript.',\n",
    "                    'Focus ONLY on analyst questions, tone, and reactions to management responses.- Ignore management‚Äôs prepared remarks unless directly referenced by analysts. - Pay attention to how analysts compare results to prior earnings calls and to market expectations.',\n",
    "                    'Use a CONSISTENT STANDARD across all earnings calls, regardless of sector, size, or company specifics.',\n",
    "                    'Evaluate (internally, equal weight) the following aspects:',\n",
    "                    'a) Guidance/Financial Outlook (revenue, earnings, margins, cash flow)',\n",
    "                    'b) Strategy/Product/Innovation (new initiatives, products, technologies)',\n",
    "                    'c) Competitive/Market Positioning (market share, competition, macro trends)',\n",
    "                    'd) Management Execution & Credibility (track record, transparency, consistency)',\n",
    "                    'Compute ONE overall analyst sentiment score on a 1-10 scale, where:',\n",
    "                    '1 = Extremely pessimistic, 5 = Neutral/mixed, 10 = Extremely optimistic.',\n",
    "                    'e) create a column with wonderful emojis that reflect the sentiment score',\n",
    "                    'Return your response as valid JSON with this exact format:',\n",
    "                    '{\"score\": <integer 1-10>, \"reason\": <brief explanation covering the four aspects>, \"analyst_count\": <integer number of unique analysts>}',\n",
    "                    'Transcript:',\n",
    "                    transcript\n",
    "                )\n",
    "            ) AS ai_response\n",
    "        FROM unique_transcripts\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        (TRY_PARSE_JSON(ai_response):score)::INT AS sentiment_score,\n",
    "        (TRY_PARSE_JSON(ai_response):analyst_count)::INT AS unique_analyst_count,\n",
    "        (TRY_PARSE_JSON(ai_response):reason)::STRING AS sentiment_reason\n",
    "    FROM ai_analysis\n",
    ");\"\"\"\n",
    "\n",
    "# --- Fetch Real Transcripts from Snowflake ---\n",
    "def fetch_transcripts_from_snowflake():\n",
    "    \"\"\"\n",
    "    Fetches actual transcripts from the UNIQUE_TRANSCRIPTS table in Snowflake\n",
    "    and converts the JSON format into readable text for preview.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Query the UNIQUE_TRANSCRIPTS table\n",
    "    transcripts_df = session.table('UNIQUE_TRANSCRIPTS').select(\n",
    "        'PRIMARY_TICKER', 'EVENT_TIMESTAMP', 'TRANSCRIPT'\n",
    "    ).limit(5).to_pandas()\n",
    "    \n",
    "    # Parse the JSON transcripts to extract readable text\n",
    "    def parse_transcript_json(transcript_json_str):\n",
    "        \"\"\"Parse the JSON transcript and convert to readable text format.\"\"\"\n",
    "        try:\n",
    "            transcript_data = json.loads(transcript_json_str)\n",
    "            parsed_transcript = transcript_data.get('parsed_transcript', [])\n",
    "            speaker_mapping = {s['speaker']: s['speaker_data'] for s in transcript_data.get('speaker_mapping', [])}\n",
    "            \n",
    "            # Build readable transcript\n",
    "            text_parts = []\n",
    "            for entry in parsed_transcript[:15]:  # Limit to first 15 exchanges for preview\n",
    "                speaker_id = entry.get('speaker')\n",
    "                text = entry.get('text', '')\n",
    "                speaker_info = speaker_mapping.get(speaker_id, {})\n",
    "                speaker_name = speaker_info.get('name', f'Speaker {speaker_id}')\n",
    "                speaker_role = speaker_info.get('role', '')\n",
    "                \n",
    "                if speaker_role:\n",
    "                    text_parts.append(f\"{speaker_name} ({speaker_role}): {text}\")\n",
    "                else:\n",
    "                    text_parts.append(f\"{speaker_name}: {text}\")\n",
    "            \n",
    "            return '\\n\\n'.join(text_parts) + '\\n\\n[... transcript continues ...]'\n",
    "        except Exception as e:\n",
    "            return str(transcript_json_str)[:500]  # Fallback to raw text preview\n",
    "    \n",
    "    # Convert JSON transcripts to readable format\n",
    "    transcripts_df['TRANSCRIPT_TEXT'] = transcripts_df['TRANSCRIPT'].apply(parse_transcript_json)\n",
    "    \n",
    "    return transcripts_df\n",
    "\n",
    "# --- Dummy Data for Preview (Now using real Snowflake data) ---\n",
    "made_up_transcripts_list = []\n",
    "\n",
    "def create_dummy_source_preview():\n",
    "    \"\"\"\n",
    "    Creates a DataFrame showing actual source transcript data from Snowflake.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch real transcripts from Snowflake\n",
    "        transcripts_df = fetch_transcripts_from_snowflake()\n",
    "        \n",
    "        # Create preview dataframe with readable format\n",
    "        preview_data = {\n",
    "            'PRIMARY_TICKER': transcripts_df['PRIMARY_TICKER'],\n",
    "            'EVENT_TIMESTAMP': transcripts_df['EVENT_TIMESTAMP'],\n",
    "            'TRANSCRIPT': transcripts_df['TRANSCRIPT_TEXT']\n",
    "        }\n",
    "        return pd.DataFrame(preview_data)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching transcripts: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Streamlit App UI ---\n",
    "\n",
    "st.markdown('''## üìù Cortex AISQL: Turn your Unstructured data into Structured\n",
    "\n",
    "**AI-based sentiment dataset** for the **Dow Jones 30 earnings calls** (2024+).\n",
    "- **Source**: annotated Q&A transcripts from `COMPANY_EVENT_TRANSCRIPT_ATTRIBUTES_V2`.\n",
    "- **AI Scoring**: Claude (`claude-4-sonnet`) analyzed only **analyst questions/tone**, ignoring management‚Äôs scripted remarks.\n",
    "- **Output**: JSON with\n",
    "  - `sentiment_score` (1‚Äì10 scale),\n",
    "  - `reason` (brief justification),\n",
    "  - `analyst_count` (unique analysts).\n",
    "\n",
    "''')\n",
    "\n",
    "with st.expander(\"Illustrative Source Data Preview\", expanded=True):\n",
    "    st.markdown(\"This is a preview of the **source transcript data** for the first 5 tickers. The AISQL query below is run against this type of unstructured data to generate the final analysis. This example is based on transcript structures found in the **Snowflake Public Data for Financial Services**.\")\n",
    "    dummy_df = create_dummy_source_preview()\n",
    "    st.dataframe(dummy_df, use_container_width=True)\n",
    "\n",
    "    st.markdown(\"--- \\n **Example Transcript Detail:**\")\n",
    "    if not dummy_df.empty:\n",
    "        example_transcript = dummy_df.iloc[0]['TRANSCRIPT']\n",
    "        st.text_area(\"Full Transcript Example\", value=example_transcript, height=400, disabled=True, label_visibility=\"collapsed\")\n",
    "    else:\n",
    "        st.info(\"No transcript data available to display.\")\n",
    "\n",
    "\n",
    "st.subheader(\"AISQL Data Generation Code\")\n",
    "st.markdown(\"The data in the `ai_transcripts_analysts_sentiments` table was generated using the following AISQL query, which uses a GenAI model to analyze each transcript.\")\n",
    "st.code(aisql_query, language='sql')\n",
    "\n",
    "start_button = st.button(\"‚ùÑÔ∏è Generate Earnings Call Sentiment Scores\", use_container_width=True)\n",
    "\n",
    "if start_button:\n",
    "    with st.spinner('Connecting to Snowflake and fetching live data...'):\n",
    "        # --- REAL STREAMLIT-IN-SNOWFLAKE CODE ---\n",
    "        # The session is already active from the top of the script.\n",
    "        \n",
    "        # Use the list of tickers we already fetched and stored in the session state.\n",
    "        tickers_list = st.session_state.tickers_list\n",
    "\n",
    "        # Fetch the full data for those 10 tickers.\n",
    "        results_df = session.table(\"ai_transcripts_analysts_sentiments\").drop('ai_response') \\\n",
    "                                .filter(col(\"PRIMARY_TICKER\").in_(tickers_list)) \\\n",
    "                                .to_pandas()\n",
    "    \n",
    "    # --- ERROR HANDLING: Check if the DataFrame is empty ---\n",
    "    if not results_df.empty:\n",
    "        st.subheader(\"Live Analyst Sentiment Analysis Results\")\n",
    "        st.markdown(\"Data loaded from the `ai_transcripts_analysts_sentiments` table.\")\n",
    "        \n",
    "        # --- Dashboard Metrics ---\n",
    "        avg_score = results_df['SENTIMENT_SCORE'].mean()\n",
    "        total_analysts = results_df['UNIQUE_ANALYST_COUNT'].sum()\n",
    "        most_positive_ticker = results_df.loc[results_df['SENTIMENT_SCORE'].idxmax()]['PRIMARY_TICKER']\n",
    "\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        col1.metric(\"Average Sentiment Score\", f\"{avg_score:.1f} / 10\")\n",
    "        col2.metric(\"Total Analysts Covered\", f\"{total_analysts}\")\n",
    "        col3.metric(\"Most Positive Ticker\", most_positive_ticker)\n",
    "\n",
    "        # --- Results Table ---\n",
    "        \n",
    "        # Function to add emojis based on score\n",
    "        def score_to_emoji(score):\n",
    "            if score >= 8:\n",
    "                return 'üöÄ'\n",
    "            elif score >= 6:\n",
    "                return 'üëç'\n",
    "            elif score >= 4:\n",
    "                return 'ü§î'\n",
    "            else:\n",
    "                return 'üìâ'\n",
    "\n",
    "        # Create the 'EMOJI' column\n",
    "        results_df['EMOJI'] = results_df['SENTIMENT_SCORE'].apply(score_to_emoji)\n",
    "        \n",
    "        # Function to apply the font-size style\n",
    "        def double_font_size(val):\n",
    "            return 'font-size: 400%'\n",
    "\n",
    "        # Apply style to the 'EMOJI' column using .map and display\n",
    "        st.dataframe(\n",
    "            results_df.style.map(double_font_size, subset=['EMOJI']),\n",
    "            use_container_width=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        st.warning(\"No data found in the table for the selected tickers. The table might be empty or still processing.\")\n",
    "\n",
    "else:\n",
    "    st.info(\"Click the button above to connect to Snowflake and load the live sentiment data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd4e61-7dd7-49d1-8d0c-aa34ca97a60e",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "DOW_Sentiment_Analysis"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Get the active Snowpark session\n",
    "session = get_active_session()\n",
    "\n",
    "st.markdown(\"## üéØ Create Analyst Sentiment Table\")\n",
    "st.markdown(\"\"\"\n",
    "This cell will create the `ai_transcripts_analysts_sentiments` table by analyzing \n",
    "earnings call transcripts from the `unique_transcripts` table using Cortex AI.\n",
    "\"\"\")\n",
    "\n",
    "# Define the SQL query\n",
    "create_table_sql = \"\"\"\n",
    "-- create analyst sentiment for DOW JONES 30 company \n",
    "-- Note timestamp is in UTC\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS ai_transcripts_analysts_sentiments AS (\n",
    "WITH \n",
    "parsed_transcripts AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        transcript,\n",
    "        -- Parse the JSON transcript and extract speaker mapping\n",
    "        PARSE_JSON(transcript) AS transcript_json\n",
    "    FROM unique_transcripts\n",
    "),\n",
    "formatted_transcripts AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        -- Build formatted transcript with speaker labels\n",
    "        LISTAGG(\n",
    "            CASE \n",
    "                WHEN speaker_data.value:speaker_data.role::STRING IS NOT NULL \n",
    "                THEN speaker_data.value:speaker_data.name::STRING || ' (' || speaker_data.value:speaker_data.role::STRING || '): ' || parsed_entry.value:text::STRING\n",
    "                ELSE speaker_data.value:speaker_data.name::STRING || ': ' || parsed_entry.value:text::STRING\n",
    "            END,\n",
    "            '\\n\\n'\n",
    "        ) WITHIN GROUP (ORDER BY parsed_entry.index) AS transcript_text\n",
    "    FROM parsed_transcripts,\n",
    "    LATERAL FLATTEN(input => transcript_json:parsed_transcript) parsed_entry,\n",
    "    LATERAL FLATTEN(input => transcript_json:speaker_mapping) speaker_data\n",
    "    WHERE speaker_data.value:speaker::INT = parsed_entry.value:speaker::INT\n",
    "    GROUP BY primary_ticker, event_timestamp, event_type, created_at\n",
    "),\n",
    "ai_analysis AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        transcript_text,\n",
    "        ai_complete(\n",
    "            'claude-4-sonnet',\n",
    "            CONCAT_WS('\\\\n',\n",
    "                'You are analyzing sell side and buy side analysts sentiment in a public company''s earnings call transcript.',\n",
    "                'Focus ONLY on analyst questions, tone, and reactions to management responses. Ignore management''s prepared remarks unless directly referenced by analysts.',\n",
    "                'Pay attention to how analysts compare results to prior earnings calls and to market expectations.',\n",
    "                'Use a CONSISTENT STANDARD across all earnings calls, regardless of sector, size, or company specifics.',\n",
    "                'Evaluate (internally, equal weight) the following aspects:',\n",
    "                'a) Guidance/Financial Outlook (revenue growth, earnings, margins, bookings, cash flow)',\n",
    "                'b) Strategy/Product/Innovation (new initiatives, products, technologies, or business models)',\n",
    "                'c) Competitive/Market Positioning (market share, competition, regulation, macro/sector trends)',\n",
    "                'd) Management Execution & Credibility (track record, transparency, quality of responses, consistency with prior guidance)',\n",
    "                'Compute ONE overall analyst sentiment score on a 1-10 scale, where:',\n",
    "                '1 = Extremely pessimistic/concerned, 5 = Neutral/mixed, 10 = Extremely optimistic/bullish.',\n",
    "                'e) A wonderful emoji that reflects the sentiment score',\n",
    "                'Return your response as valid JSON with this exact format:',\n",
    "                '{\"score\": <integer 1-10>, \"emoji\":\"<an emoji>\",\"reason\": \"<brief explanation covering the four aspects>\", \"analyst_count\": <integer number of unique analysts>}',\n",
    "                '',\n",
    "                'IMPORTANT: Only analyze the parts where Analysts are speaking. Look for speaker labels like \"Analyst\" in the transcript below.',\n",
    "                '',\n",
    "                'Transcript:',\n",
    "                transcript_text\n",
    "            )\n",
    "        ) AS ai_response\n",
    "    FROM formatted_transcripts\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    ai_response,\n",
    "    primary_ticker,\n",
    "    event_timestamp,\n",
    "    event_type,\n",
    "    created_at,\n",
    "    /* Safely parse JSON. If parsing fails, these will be NULL. */\n",
    "    (TRY_PARSE_JSON(ai_response):emoji)::TEXT      AS emoji,\n",
    "    (TRY_PARSE_JSON(ai_response):score)::INT      AS sentiment_score,\n",
    "    (TRY_PARSE_JSON(ai_response):analyst_count)::INT AS unique_analyst_count,\n",
    "    (TRY_PARSE_JSON(ai_response):reason)::STRING  AS sentiment_reason\n",
    "FROM ai_analysis\n",
    "-- where unique_analyst_count > 1 -- remove data that only contains speakers from the public company\n",
    "ORDER BY primary_ticker, event_timestamp\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Show the SQL\n",
    "st.code(create_table_sql, language='sql')\n",
    "\n",
    "# Button to execute\n",
    "if st.button(\"üöÄ Create/Refresh Sentiment Table\", use_container_width=True):\n",
    "    with st.spinner(\"Creating table and analyzing transcripts with Cortex AI... This may take several minutes.\"):\n",
    "        try:\n",
    "            # Execute the CREATE TABLE statement\n",
    "            session.sql(create_table_sql).collect()\n",
    "            st.success(\"‚úÖ Table `ai_transcripts_analysts_sentiments` created successfully!\")\n",
    "            \n",
    "            # Query the results\n",
    "            results_df = session.sql(\"SELECT * FROM ai_transcripts_analysts_sentiments\").to_pandas()\n",
    "            \n",
    "            st.subheader(f\"üìä Results: {len(results_df)} rows\")\n",
    "            st.dataframe(results_df, use_container_width=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error creating table: {str(e)}\")\n",
    "else:\n",
    "    st.info(\"üëÜ Click the button above to create the sentiment analysis table.\")\n",
    "    \n",
    "    # Check if table already exists and show preview\n",
    "    try:\n",
    "        existing_count = session.sql(\"SELECT COUNT(*) as cnt FROM ai_transcripts_analysts_sentiments\").collect()\n",
    "        if existing_count:\n",
    "            count = existing_count[0]['CNT']\n",
    "            st.info(f\"‚ÑπÔ∏è Table already exists with {count} rows. Click the button to refresh.\")\n",
    "            \n",
    "            # Show preview\n",
    "            preview_df = session.sql(\"SELECT * FROM ai_transcripts_analysts_sentiments LIMIT 10\").to_pandas()\n",
    "            st.dataframe(preview_df, use_container_width=True)\n",
    "    except:\n",
    "        st.warning(\"‚ö†Ô∏è Table does not exist yet. Click the button to create it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1713de",
   "metadata": {
    "language": "python",
    "name": "debug_transcript_parsing"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "st.markdown(\"## üîç Debug Transcript Parsing\")\n",
    "st.markdown(\"This cell helps debug how transcripts are being parsed.\")\n",
    "\n",
    "# Test query to see what we're getting\n",
    "debug_sql = \"\"\"\n",
    "WITH \n",
    "parsed_transcripts AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        transcript,\n",
    "        PARSE_JSON(transcript) AS transcript_json\n",
    "    FROM unique_transcripts\n",
    "    LIMIT 1\n",
    "),\n",
    "flattened_entries AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        parsed_entry.index AS entry_index,\n",
    "        parsed_entry.value:speaker::INT AS speaker_id,\n",
    "        parsed_entry.value:text::STRING AS entry_text\n",
    "    FROM parsed_transcripts,\n",
    "    LATERAL FLATTEN(input => transcript_json:parsed_transcript) parsed_entry\n",
    "),\n",
    "speaker_info AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        speaker_data.value:speaker::INT AS speaker_id,\n",
    "        speaker_data.value:speaker_data.name::STRING AS speaker_name,\n",
    "        speaker_data.value:speaker_data.role::STRING AS speaker_role\n",
    "    FROM parsed_transcripts,\n",
    "    LATERAL FLATTEN(input => transcript_json:speaker_mapping) speaker_data\n",
    ")\n",
    "SELECT \n",
    "    f.primary_ticker,\n",
    "    f.entry_index,\n",
    "    s.speaker_name,\n",
    "    s.speaker_role,\n",
    "    LEFT(f.entry_text, 100) AS text_preview,\n",
    "    LENGTH(f.entry_text) AS text_length\n",
    "FROM flattened_entries f\n",
    "JOIN speaker_info s ON f.primary_ticker = s.primary_ticker AND f.speaker_id = s.speaker_id\n",
    "ORDER BY f.entry_index\n",
    "\"\"\"\n",
    "\n",
    "st.code(debug_sql, language='sql')\n",
    "\n",
    "if st.button(\"üîç Run Debug Query\", use_container_width=True):\n",
    "    with st.spinner(\"Analyzing transcript structure...\"):\n",
    "        try:\n",
    "            result_df = session.sql(debug_sql).to_pandas()\n",
    "            \n",
    "            st.success(f\"‚úÖ Found {len(result_df)} transcript entries\")\n",
    "            \n",
    "            # Show summary\n",
    "            ticker = result_df.iloc[0]['PRIMARY_TICKER'] if len(result_df) > 0 else 'N/A'\n",
    "            analyst_count = len(result_df[result_df['SPEAKER_ROLE'] == 'Analyst'])\n",
    "            \n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            col1.metric(\"Ticker\", ticker)\n",
    "            col2.metric(\"Total Entries\", len(result_df))\n",
    "            col3.metric(\"Analyst Entries\", analyst_count)\n",
    "            \n",
    "            # Show the data\n",
    "            st.dataframe(result_df, use_container_width=True)\n",
    "            \n",
    "            # Now test the LISTAGG\n",
    "            st.subheader(\"Testing LISTAGG Concatenation\")\n",
    "            \n",
    "            listagg_sql = \"\"\"\n",
    "            WITH \n",
    "            parsed_transcripts AS (\n",
    "                SELECT\n",
    "                    primary_ticker,\n",
    "                    PARSE_JSON(transcript) AS transcript_json\n",
    "                FROM unique_transcripts\n",
    "                LIMIT 1\n",
    "            )\n",
    "            SELECT\n",
    "                primary_ticker,\n",
    "                LISTAGG(\n",
    "                    CASE \n",
    "                        WHEN speaker_data.value:speaker_data.role::STRING IS NOT NULL \n",
    "                        THEN speaker_data.value:speaker_data.name::STRING || ' (' || speaker_data.value:speaker_data.role::STRING || '): ' || parsed_entry.value:text::STRING\n",
    "                        ELSE speaker_data.value:speaker_data.name::STRING || ': ' || parsed_entry.value:text::STRING\n",
    "                    END,\n",
    "                    '\\\\n\\\\n'\n",
    "                ) WITHIN GROUP (ORDER BY parsed_entry.index) AS transcript_text,\n",
    "                LENGTH(LISTAGG(\n",
    "                    CASE \n",
    "                        WHEN speaker_data.value:speaker_data.role::STRING IS NOT NULL \n",
    "                        THEN speaker_data.value:speaker_data.name::STRING || ' (' || speaker_data.value:speaker_data.role::STRING || '): ' || parsed_entry.value:text::STRING\n",
    "                        ELSE speaker_data.value:speaker_data.name::STRING || ': ' || parsed_entry.value:text::STRING\n",
    "                    END,\n",
    "                    '\\\\n\\\\n'\n",
    "                ) WITHIN GROUP (ORDER BY parsed_entry.index)) AS total_length\n",
    "            FROM parsed_transcripts,\n",
    "            LATERAL FLATTEN(input => transcript_json:parsed_transcript) parsed_entry,\n",
    "            LATERAL FLATTEN(input => transcript_json:speaker_mapping) speaker_data\n",
    "            WHERE speaker_data.value:speaker::INT = parsed_entry.value:speaker::INT\n",
    "            GROUP BY primary_ticker\n",
    "            \"\"\"\n",
    "            \n",
    "            listagg_result = session.sql(listagg_sql).to_pandas()\n",
    "            \n",
    "            if len(listagg_result) > 0:\n",
    "                total_length = listagg_result.iloc[0]['TOTAL_LENGTH']\n",
    "                transcript_text = listagg_result.iloc[0]['TRANSCRIPT_TEXT']\n",
    "                \n",
    "                st.metric(\"Concatenated Transcript Length\", f\"{total_length:,} characters\")\n",
    "                \n",
    "                # Show first 2000 chars\n",
    "                st.text_area(\"First 2000 characters of formatted transcript:\", \n",
    "                           value=transcript_text[:2000], \n",
    "                           height=300)\n",
    "                \n",
    "                # Show last 2000 chars to see if we have Q&A\n",
    "                st.text_area(\"Last 2000 characters of formatted transcript:\", \n",
    "                           value=transcript_text[-2000:] if len(transcript_text) > 2000 else transcript_text, \n",
    "                           height=300)\n",
    "                \n",
    "                # Check for analyst mentions\n",
    "                analyst_mentions = transcript_text.count('(Analyst)')\n",
    "                st.metric(\"Number of '(Analyst)' labels found\", analyst_mentions)\n",
    "                \n",
    "                if analyst_mentions == 0:\n",
    "                    st.error(\"‚ö†Ô∏è No analyst labels found in the concatenated transcript!\")\n",
    "                else:\n",
    "                    st.success(f\"‚úÖ Found {analyst_mentions} analyst entries in the transcript\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "else:\n",
    "    st.info(\"üëÜ Click the button above to debug transcript parsing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5076d-2c0b-4927-96bb-264d12ea6e92",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "live_stock_data"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import altair as alt\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# --- Title and Introduction ---\n",
    "st.title(\"‚ùÑÔ∏è Live FSI Data from Snowflake for Financial Services\")\n",
    "st.markdown(\n",
    "    \"This app runs directly in Snowflake and uses the data from the **Snowflake Marketplace**.\"\n",
    ")\n",
    "\n",
    "# --- Get Active Session and Query Data ---\n",
    "# Get the current session object provided by the Streamlit in Snowflake environment\n",
    "session = get_active_session()\n",
    "\n",
    "# This query now calculates the offset needed to make the latest date appear as '2025-10-09'\n",
    "query = \"\"\"\n",
    "    WITH DateOffset AS (\n",
    "        -- First, calculate the difference in days between the absolute latest date\n",
    "        -- in the table and our target date of Oct 9, 2025.\n",
    "        SELECT\n",
    "            DATEDIFF(\n",
    "                day,\n",
    "                MAX(TO_TIMESTAMP_NTZ(DATE, 9)::DATE),\n",
    "                '2025-10-09'::DATE\n",
    "            ) AS days_to_add\n",
    "        FROM SWT_LONDON_2025.FIN_SERV.FSI_DATA\n",
    "    )\n",
    "    -- Now, apply this calculated offset to every date in the result set.\n",
    "    SELECT\n",
    "        TICKER,\n",
    "        ASSET_CLASS,\n",
    "        PRIMARY_EXCHANGE_NAME,\n",
    "        DATEADD(\n",
    "            day,\n",
    "            (SELECT days_to_add FROM DateOffset),\n",
    "            TO_TIMESTAMP_NTZ(DATE, 9)::DATE\n",
    "        ) as DATE,\n",
    "        PRICE,\n",
    "        RETURN,\n",
    "        IS_SPLIT,\n",
    "        Y\n",
    "    FROM SWT_LONDON_2025.FIN_SERV.FSI_DATA\n",
    "    ORDER BY DATE DESC\n",
    "    LIMIT 500;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query using the Snowpark session and convert to a Pandas DataFrame\n",
    "data_frame = session.sql(query).to_pandas()\n",
    "\n",
    "\n",
    "# --- Display the \"Nice Looking\" Dataframe ---\n",
    "st.header(\"FSI Data Explorer\")\n",
    "st.dataframe(\n",
    "    data_frame,\n",
    "    use_container_width=True,\n",
    "    hide_index=True,\n",
    "    column_config={\n",
    "        \"TICKER\": st.column_config.TextColumn(\n",
    "            \"Ticker Symbol\",\n",
    "            help=\"The stock ticker symbol.\",\n",
    "            width=\"small\"\n",
    "        ),\n",
    "        \"ASSET_CLASS\": st.column_config.TextColumn(\n",
    "            \"Asset Class\",\n",
    "            width=\"small\"\n",
    "        ),\n",
    "        \"PRIMARY_EXCHANGE_NAME\": \"Primary Exchange\",\n",
    "        \"DATE\": st.column_config.DateColumn(\n",
    "            \"Date\",\n",
    "            format=\"DD MMM, YYYY\" # Format for readability\n",
    "        ),\n",
    "        \"PRICE\": st.column_config.NumberColumn(\n",
    "            \"Price (USD)\",\n",
    "            help=\"Closing price in US Dollars.\",\n",
    "            format=\"$%.2f\"\n",
    "        ),\n",
    "        \"RETURN\": st.column_config.NumberColumn(\n",
    "            \"Daily Return\",\n",
    "            help=\"The daily percentage return.\",\n",
    "            format=\"%.2f%%\"\n",
    "        ),\n",
    "        \"IS_SPLIT\": st.column_config.CheckboxColumn(\n",
    "            \"Stock Split?\",\n",
    "            help=\"Indicates if a stock split occurred.\",\n",
    "            width=\"small\"\n",
    "        ),\n",
    "        \"Y\": st.column_config.ProgressColumn(\n",
    "            \"Target Probability (Y)\",\n",
    "            help=\"Model prediction or target variable.\",\n",
    "            format=\"%.3f\",\n",
    "            min_value=0,\n",
    "            max_value=1,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- NEW SECTION: Daily Price Charts ---\n",
    "st.header(\"Daily Price Charts for Sample Tickers\")\n",
    "\n",
    "# Get unique tickers from the dataframe\n",
    "all_tickers = data_frame['TICKER'].unique().tolist()\n",
    "\n",
    "# Select up to the first 6 tickers to plot (no price filtering)\n",
    "tickers_to_plot = all_tickers[:6]\n",
    "\n",
    "# Only create the columns and charts if we found any tickers\n",
    "if tickers_to_plot:\n",
    "    # Create a column for each ticker we are plotting\n",
    "    cols = st.columns(len(tickers_to_plot))\n",
    "\n",
    "    # Iterate through the columns and the filtered tickers to create a chart in each column\n",
    "    for i, ticker in enumerate(tickers_to_plot):\n",
    "        with cols[i]:\n",
    "            st.subheader(ticker)\n",
    "            \n",
    "            # Filter the dataframe for the current ticker\n",
    "            ticker_data = data_frame[data_frame[\"TICKER\"] == ticker]\n",
    "            \n",
    "            # Calculate the min and max price for *this specific ticker* to set a dynamic y-axis\n",
    "            min_price = ticker_data['PRICE'].min()\n",
    "            max_price = ticker_data['PRICE'].max()\n",
    "            \n",
    "            # Use Altair to create a chart with a custom y-axis and Snowflake blue line\n",
    "            chart = alt.Chart(ticker_data).mark_line(color='#00B2EE').encode( # Snowflake blue color added here\n",
    "                x=alt.X('DATE', title='Date'),\n",
    "                y=alt.Y('PRICE', title='Price', scale=alt.Scale(domain=[min_price - (min_price*0.05), max_price + (max_price*0.05)])), # Dynamic domain based on min/max price\n",
    "                tooltip=['DATE', 'PRICE']\n",
    "            ).interactive()\n",
    "            \n",
    "            # Display the Altair chart\n",
    "            st.altair_chart(chart, use_container_width=True)\n",
    "else:\n",
    "    st.info(\"No tickers found in this data sample to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e10a5-2105-4294-a7cb-bc3ef331ed7c",
   "metadata": {
    "collapsed": false,
    "name": "Data_Science_Agent"
   },
   "source": [
    "# Data Science Agent: Financial ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbade0-81b6-4490-8f0c-3289e756928f",
   "metadata": {
    "collapsed": false,
    "name": "prompt1"
   },
   "source": [
    "# Prompt 1 - \n",
    "\n",
    "Help me construct features with returns, eg. the last 1 day return using close price, return from **t-4** to **t-1**, return from **t-9** to **t-5**, return from **t-20** to **t-11**, and return from **t-62** to **t-21**. I also want to construct my predictive variable, which should be the future return from **t+2** to **t+6**. Please also help take the log across all return variables, including the previously constructed feature and this predictive variable. Please keep as panel data where ticker is a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ee63b-b13a-494d-b6ce-6a95b84f47a3",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "Feature_Engineering"
   },
   "outputs": [],
   "source": [
    "# Generated by Snowflake Copilot\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get session\n",
    "session = get_active_session()\n",
    "\n",
    "# Get the data with necessary columns\n",
    "data = session.table('SWT_LONDON_2025.FIN_SERV.FSI_DATA') \\\n",
    "    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n",
    "             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n",
    "    .to_pandas()\n",
    "\n",
    "# Calculate future return (t+2 to t+5)\n",
    "data['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n",
    "                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n",
    "\n",
    "# Apply log transformation to all return columns\n",
    "return_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\n",
    "for col in return_cols:\n",
    "    data[f'LOG_{col}'] = np.log(1 + data[col])\n",
    "\n",
    "# Select final columns\n",
    "final_columns = ['TICKER', 'DATE'] + [f'LOG_{col}' for col in return_cols]\n",
    "final_data = data[final_columns]\n",
    "\n",
    "# Sort by ticker and date\n",
    "final_data = final_data.sort_values(['TICKER', 'DATE'])\n",
    "\n",
    "print(f\"Final dataset shape: {final_data.shape}\")\n",
    "print(\"\\nFirst few rows of the processed data:\")\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f71e32-d13e-4d36-ba29-e7206797482f",
   "metadata": {
    "collapsed": false,
    "name": "prompt_2"
   },
   "source": [
    "# Prompt 2\n",
    "\n",
    "With those features, I want to train a predictive **lightGBM** model with **L2 metric**. Please do a walk forward training on a quarterly basis. For each test quarter:\n",
    "- **Train** on all quarters \\< (Q-2)\n",
    "- **Validate** on (Q-2, Q-1)\n",
    "- **Test** on **Q**\n",
    "- Enforce strict cutoffs so rows needing returns beyond the split end are dropped (no look-ahead).\n",
    "\n",
    "\n",
    "### The Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b75a5-ccc5-43ed-a014-38ca9f03b5e9",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "Training_ML_Model"
   },
   "outputs": [],
   "source": [
    "# Generated by Snowflake Copilot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Get session and data\n",
    "session = get_active_session()\n",
    "data = session.table('SWT_LONDON_2025.FIN_SERV.FSI_DATA') \\\n",
    "    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n",
    "             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n",
    "    .to_pandas()\n",
    "\n",
    "# Calculate future return and log transform returns\n",
    "data['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n",
    "                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n",
    "\n",
    "# Log transform all return columns\n",
    "return_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\n",
    "for col in return_cols:\n",
    "    data[f'LOG_{col}'] = np.log(1 + data[col])\n",
    "\n",
    "# Create quarter information\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\n",
    "target_col = 'LOG_FUTURE_RETURN_2_5'\n",
    "\n",
    "# Get all quarters from 2020Q1 onwards\n",
    "all_quarters = sorted([q for q in data['YEAR_QUARTER'].unique() if q >= pd.Period('2020Q1', freq='Q')])\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Walk-forward testing with improved model parameters\n",
    "for test_quarter in all_quarters:\n",
    "    # Define validation quarters (Q-2, Q-1)\n",
    "    val_quarters = [test_quarter - 2, test_quarter - 1]\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = data[data['YEAR_QUARTER'] < val_quarters[0]]\n",
    "    val_data = data[data['YEAR_QUARTER'].isin(val_quarters)]\n",
    "    test_data = data[data['YEAR_QUARTER'] == test_quarter]\n",
    "    \n",
    "    # Create datasets\n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data[target_col]\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_val = val_data[target_col]\n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data[target_col]\n",
    "    \n",
    "    # Train model with improved parameters\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='l2',\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train with validation set\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    \n",
    "    # Make predictions\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, test_pred)\n",
    "    r2 = 1 - np.sum((y_test - test_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'quarter': test_quarter,\n",
    "        'train_size': len(X_train),\n",
    "        'val_size': len(X_val),\n",
    "        'test_size': len(X_test),\n",
    "        'mse': mse,\n",
    "        'r2': r2\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(\"Walk-forward testing results:\")\n",
    "for r in results:\n",
    "    print(f\"\\nQuarter: {r['quarter']}\")\n",
    "    print(f\"Train size: {r['train_size']}, Val size: {r['val_size']}, Test size: {r['test_size']}\")\n",
    "    print(f\"MSE: {r['mse']:.6f}\")\n",
    "    print(f\"R2: {r['r2']:.6f}\")\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_mse = np.mean([r['mse'] for r in results])\n",
    "avg_r2 = np.mean([r['r2'] for r in results])\n",
    "print(f\"\\nAverage MSE across all quarters: {avg_mse:.6f}\")\n",
    "print(f\"Average R2 across all quarters: {avg_r2:.6f}\")\n",
    "\n",
    "# Save predictions for the most recent quarter\n",
    "last_quarter = all_quarters[-1]\n",
    "last_quarter_data = data[data['YEAR_QUARTER'] == last_quarter].copy()\n",
    "X_last = last_quarter_data[feature_cols]\n",
    "last_quarter_data['predicted_return'] = model.predict(X_last)\n",
    "final_predictions = last_quarter_data[['TICKER', 'DATE', target_col, 'predicted_return']]\n",
    "print(\"\\nSample of final predictions for the last quarter:\")\n",
    "print(final_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a89cc-7497-476f-b53c-94629bb41839",
   "metadata": {
    "collapsed": false,
    "name": "prompt_3"
   },
   "source": [
    "## Prompt 3\n",
    "\n",
    "Let's test if the strategy can work, starting 2021, for each portfolio construction, generate forecasts on **Tuesdays** in the test quarter.\n",
    "- At **Wednesday close**, go **long top-5** and **short bottom-5** by predicted return (equal weight).\n",
    "- Hold through **Thu ‚Üí next Wed** (exactly the `t+2..t+6` window used for training).\n",
    "- **Transaction cost**: 3.0 bps one-way applied via weekly turnover.\n",
    "Show Strategy metrics:\n",
    "- Information Ratio (before/after costs)\n",
    "- Max drawdown\n",
    "- At the end plot the equity curve on before & after cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9dcd9-7bc9-4705-9aea-8d5e64237db0",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "Backtesting"
   },
   "outputs": [],
   "source": [
    "# Generated by Snowflake Copilot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get session and data\n",
    "session = get_active_session()\n",
    "data = session.table('SWT_LONDON_2025.FIN_SERV.FSI_DATA') \\\n",
    "    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n",
    "             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n",
    "    .to_pandas()\n",
    "\n",
    "# Calculate future return and log transform\n",
    "data['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n",
    "                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n",
    "\n",
    "return_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\n",
    "for col in return_cols:\n",
    "    data[f'LOG_{col}'] = np.log(1 + data[col])\n",
    "\n",
    "# Create quarter and day information\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\n",
    "data['DAY_OF_WEEK'] = data['DATE'].dt.day_name()\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\n",
    "target_col = 'LOG_FUTURE_RETURN_2_5'\n",
    "\n",
    "# Get quarters from 2021Q1 onwards for strategy testing\n",
    "strategy_quarters = sorted([q for q in data['YEAR_QUARTER'].unique() if q >= pd.Period('2021Q1', freq='Q')])\n",
    "\n",
    "# Initialize storage for all predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Walk-forward testing starting from 2021Q1\n",
    "for test_quarter in strategy_quarters:\n",
    "    # Define validation quarters (Q-2, Q-1)\n",
    "    val_quarters = [test_quarter - 2, test_quarter - 1]\n",
    "    \n",
    "    # Split the data - train on all quarters < (Q-2)\n",
    "    train_data = data[data['YEAR_QUARTER'] < val_quarters[0]]\n",
    "    val_data = data[data['YEAR_QUARTER'].isin(val_quarters)]\n",
    "    test_data = data[data['YEAR_QUARTER'] == test_quarter]\n",
    "    \n",
    "    # Skip if insufficient training data\n",
    "    if len(train_data) < 1000:\n",
    "        continue\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_data[feature_cols].dropna()\n",
    "    y_train = train_data[target_col].loc[X_train.index]\n",
    "    X_val = val_data[feature_cols].dropna()\n",
    "    y_val = val_data[target_col].loc[X_val.index]\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='l2',\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
    "    \n",
    "    # Get Tuesdays in the test quarter for prediction\n",
    "    test_tuesdays = test_data[test_data['DAY_OF_WEEK'] == 'Tuesday'].copy()\n",
    "    \n",
    "    if len(test_tuesdays) > 0:\n",
    "        # Generate predictions for Tuesdays\n",
    "        X_test_tuesday = test_tuesdays[feature_cols].dropna()\n",
    "        if len(X_test_tuesday) > 0:\n",
    "            predictions = model.predict(X_test_tuesday)\n",
    "            test_tuesdays_clean = test_tuesdays.loc[X_test_tuesday.index].copy()\n",
    "            test_tuesdays_clean['predicted_return'] = predictions\n",
    "            test_tuesdays_clean['test_quarter'] = test_quarter\n",
    "            all_predictions.append(test_tuesdays_clean)\n",
    "\n",
    "# Combine all predictions\n",
    "predictions_df = pd.concat(all_predictions, ignore_index=True)\n",
    "\n",
    "# Strategy implementation\n",
    "def calculate_strategy_returns(predictions_df):\n",
    "    strategy_results = []\n",
    "    \n",
    "    # Group by date (Tuesday predictions)\n",
    "    for date, group in predictions_df.groupby('DATE'):\n",
    "        # Rank by predicted return\n",
    "        group_sorted = group.sort_values('predicted_return', ascending=False)\n",
    "        \n",
    "        # Select top 5 (long) and bottom 5 (short)\n",
    "        if len(group_sorted) >= 10:\n",
    "            long_positions = group_sorted.head(5).copy()\n",
    "            short_positions = group_sorted.tail(5).copy()\n",
    "            \n",
    "            # Calculate portfolio return (equal weight)\n",
    "            long_return = long_positions['LOG_FUTURE_RETURN_2_5'].mean()\n",
    "            short_return = short_positions['LOG_FUTURE_RETURN_2_5'].mean()\n",
    "            \n",
    "            # Long-short strategy return\n",
    "            strategy_return = long_return - short_return\n",
    "            \n",
    "            # Store results\n",
    "            strategy_results.append({\n",
    "                'date': date,\n",
    "                'long_return': long_return,\n",
    "                'short_return': short_return,\n",
    "                'strategy_return': strategy_return,\n",
    "                'num_stocks': len(group_sorted),\n",
    "                'long_tickers': list(long_positions['TICKER']),\n",
    "                'short_tickers': list(short_positions['TICKER'])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(strategy_results)\n",
    "\n",
    "# Calculate strategy returns\n",
    "strategy_df = calculate_strategy_returns(predictions_df)\n",
    "\n",
    "# Calculate transaction costs based on turnover\n",
    "def calculate_turnover_and_costs(strategy_df):\n",
    "    strategy_df = strategy_df.copy()\n",
    "    strategy_df['turnover'] = 0.0\n",
    "    strategy_df['transaction_cost'] = 0.0\n",
    "    \n",
    "    prev_long = set()\n",
    "    prev_short = set()\n",
    "    \n",
    "    for i, row in strategy_df.iterrows():\n",
    "        current_long = set(row['long_tickers'])\n",
    "        current_short = set(row['short_tickers'])\n",
    "        \n",
    "        if i > 0:\n",
    "            # Calculate turnover as fraction of positions that changed\n",
    "            long_changes = len(current_long.symmetric_difference(prev_long))\n",
    "            short_changes = len(current_short.symmetric_difference(prev_short))\n",
    "            total_changes = long_changes + short_changes\n",
    "            total_positions = 10  # 5 long + 5 short\n",
    "            \n",
    "            turnover = total_changes / total_positions\n",
    "            # Transaction cost: 3.0 bps one-way on turnover\n",
    "            transaction_cost = turnover * 0.0003  # 3.0 bps = 0.0003\n",
    "            \n",
    "            strategy_df.loc[i, 'turnover'] = turnover\n",
    "            strategy_df.loc[i, 'transaction_cost'] = transaction_cost\n",
    "        \n",
    "        prev_long = current_long\n",
    "        prev_short = current_short\n",
    "    \n",
    "    return strategy_df\n",
    "\n",
    "# Calculate costs\n",
    "strategy_df = calculate_turnover_and_costs(strategy_df)\n",
    "\n",
    "# Calculate net returns after transaction costs\n",
    "strategy_df['net_return'] = strategy_df['strategy_return'] - strategy_df['transaction_cost']\n",
    "\n",
    "# Calculate cumulative returns and strategy metrics\n",
    "strategy_df['cumulative_gross'] = (1 + strategy_df['strategy_return']).cumprod()\n",
    "strategy_df['cumulative_net'] = (1 + strategy_df['net_return']).cumprod()\n",
    "\n",
    "# Calculate Information Ratio (annualized)\n",
    "weeks_per_year = 52\n",
    "gross_annual_return = strategy_df['strategy_return'].mean() * weeks_per_year\n",
    "net_annual_return = strategy_df['net_return'].mean() * weeks_per_year\n",
    "gross_annual_vol = strategy_df['strategy_return'].std() * np.sqrt(weeks_per_year)\n",
    "net_annual_vol = strategy_df['net_return'].std() * np.sqrt(weeks_per_year)\n",
    "\n",
    "ir_gross = gross_annual_return / gross_annual_vol\n",
    "ir_net = net_annual_return / net_annual_vol\n",
    "\n",
    "# Calculate Maximum Drawdown\n",
    "def calculate_max_drawdown(cumulative_returns):\n",
    "    peak = cumulative_returns.expanding().max()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    return drawdown.min()\n",
    "\n",
    "max_dd_gross = calculate_max_drawdown(strategy_df['cumulative_gross'])\n",
    "max_dd_net = calculate_max_drawdown(strategy_df['cumulative_net'])\n",
    "\n",
    "# Print strategy metrics\n",
    "print(\"=== STRATEGY PERFORMANCE METRICS ===\")\n",
    "print(f\"\\nGross Performance (Before Costs):\")\n",
    "print(f\"  Annual Return: {gross_annual_return:.2%}\")\n",
    "print(f\"  Annual Volatility: {gross_annual_vol:.2%}\")\n",
    "print(f\"  Information Ratio: {ir_gross:.3f}\")\n",
    "print(f\"  Maximum Drawdown: {max_dd_gross:.2%}\")\n",
    "\n",
    "print(f\"\\nNet Performance (After Costs):\")\n",
    "print(f\"  Annual Return: {net_annual_return:.2%}\")\n",
    "print(f\"  Annual Volatility: {net_annual_vol:.2%}\")\n",
    "print(f\"  Information Ratio: {ir_net:.3f}\")\n",
    "print(f\"  Maximum Drawdown: {max_dd_net:.2%}\")\n",
    "\n",
    "print(f\"\\nTransaction Cost Impact:\")\n",
    "print(f\"  Cost Drag: {(gross_annual_return - net_annual_return):.2%} annually\")\n",
    "print(f\"  Average Weekly Turnover: {strategy_df['turnover'].mean():.1%}\")\n",
    "\n",
    "# Final performance summary\n",
    "total_gross_return = strategy_df['cumulative_gross'].iloc[-1] - 1\n",
    "total_net_return = strategy_df['cumulative_net'].iloc[-1] - 1\n",
    "print(f\"\\nTotal Strategy Performance ({strategy_df['date'].min().strftime('%Y-%m-%d')} to {strategy_df['date'].max().strftime('%Y-%m-%d')}):\")\n",
    "print(f\"  Gross Total Return: {total_gross_return:.2%}\")\n",
    "print(f\"  Net Total Return: {total_net_return:.2%}\")\n",
    "\n",
    "# Create equity curve plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(strategy_df['date'], strategy_df['cumulative_gross'], label='Gross Returns (Before Costs)', linewidth=2)\n",
    "plt.plot(strategy_df['date'], strategy_df['cumulative_net'], label='Net Returns (After Costs)', linewidth=2)\n",
    "plt.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "\n",
    "plt.title('Long-Short Strategy Equity Curve (2021-2025)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Cumulative Return (Base = 1)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add performance annotations\n",
    "final_gross = strategy_df['cumulative_gross'].iloc[-1]\n",
    "final_net = strategy_df['cumulative_net'].iloc[-1]\n",
    "plt.text(0.02, 0.98, f'Final Gross: {final_gross:.3f} (+{(final_gross-1)*100:.1f}%)', \n",
    "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "plt.text(0.02, 0.90, f'Final Net: {final_net:.3f} (+{(final_net-1)*100:.1f}%)', \n",
    "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Strategy implementation completed successfully!\")\n",
    "print(f\"Total trading weeks: {len(strategy_df)}\")\n",
    "print(f\"Strategy period: {strategy_df['date'].min().strftime('%Y-%m-%d')} to {strategy_df['date'].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07710bd3-98cf-46a9-a078-ca209ea22e85",
   "metadata": {
    "collapsed": false,
    "name": "prompt4"
   },
   "source": [
    "## Prompt 4\n",
    "\n",
    "Register only the final model in Snowflake Registry with the following options:\n",
    "\n",
    "- model name \"STOCK_RETURN_PREDICTOR_GBM\"\n",
    "- sample input of 100 rows\n",
    "- target_platforms=[\"WAREHOUSE\"]\n",
    "- options={\n",
    "    \"relax_version\": False,\n",
    "    \"target_methods\": [\"predict\"],\n",
    "    \"method_options\": {\n",
    "        \"predict\": {\"case_sensitive\": True}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252dfa42-8603-4ac9-a799-bfeeef78df54",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "python",
    "name": "Reg_Model"
   },
   "outputs": [],
   "source": [
    "# Generated by Snowflake Copilot\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "data = session.table('SWT_LONDON_2025.FIN_SERV.FSI_DATA') \\\n",
    "    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n",
    "             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n",
    "    .to_pandas()\n",
    "\n",
    "data['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n",
    "                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n",
    "\n",
    "return_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\n",
    "for col in return_cols:\n",
    "    data[f'LOG_{col}'] = np.log(1 + data[col])\n",
    "\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\n",
    "\n",
    "feature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\n",
    "target_col = 'LOG_FUTURE_RETURN_2_5'\n",
    "\n",
    "training_data = data[data['YEAR_QUARTER'] < pd.Period('2025Q4', freq='Q')].copy()\n",
    "training_data_clean = training_data.dropna(subset=feature_cols + [target_col])\n",
    "\n",
    "X_train = training_data_clean[feature_cols]\n",
    "y_train = training_data_clean[target_col]\n",
    "\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='l2',\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "sample_input = X_train.head(100)\n",
    "\n",
    "registry = Registry(session=session)\n",
    "\n",
    "model_ref = registry.log_model(\n",
    "    model=final_model,\n",
    "    model_name=\"STOCK_RETURN_PREDICTOR_GBM\",\n",
    "    sample_input_data=sample_input,\n",
    "    target_platforms=[\"WAREHOUSE\"],\n",
    "    options={\n",
    "        \"relax_version\": False,\n",
    "        \"target_methods\": [\"predict\"],\n",
    "        \"method_options\": {\n",
    "            \"predict\": {\"case_sensitive\": True}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model registered successfully!\")\n",
    "print(f\"Model name: {model_ref.model_name}\")\n",
    "print(f\"Model version: {model_ref.version_name}\")\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Feature columns used: {list(sample_input.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "becky.oconnor@snowflake.com",
   "authorId": "7804304340939",
   "authorName": "USER",
   "lastEditTime": 1759835069603,
   "notebookId": "zeagk6cz7vyf3m565sa7",
   "sessionId": "2a2ec128-1423-466a-bd5b-7f52491517da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

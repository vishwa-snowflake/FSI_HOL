{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.registry import Registry\n",
    "session = get_active_session()\n",
    "registry = Registry(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fe344-365c-4c11-bcde-811a7f7da61b",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "# ðŸ“ Analyst Sentiment vs Market Reaction\n",
    "\n",
    "## Sentiment Extraction  \n",
    "We built an **AI-based sentiment dataset** for the **Dow Jones 30 earnings calls** (2024+).  \n",
    "- **Source**: annotated Q&A transcripts from `COMPANY_EVENT_TRANSCRIPT_ATTRIBUTES`.  \n",
    "- **Pre-processing**: deduplicated transcripts by ticker/timestamp; kept earliest point-in-time version.  \n",
    "- **AI Scoring**: Claude (`claude-4-sonnet`) analyzed only **analyst questions/tone**, ignoring managementâ€™s scripted remarks.  \n",
    "- **Output**: JSON with  \n",
    "  - `sentiment_score` (1â€“10 scale),  \n",
    "  - `reason` (brief justification),  \n",
    "  - `analyst_count` (unique analysts).  \n",
    "- Dropped events with â‰¤1 analyst to avoid company-only remarks.\n",
    "\n",
    "## Linking Sentiment to Returns  \n",
    "We tested whether analyst sentiment predicts short-term stock moves.  \n",
    "- **Signal**:  \n",
    "  - Level and Change (compare to prev call) of the Analyst Sentiment\n",
    "\n",
    "- **Returns**:  \n",
    "  - `1D return` = reaction during the call,  \n",
    "  - `3D return` = drift after the call (`lead1+lead2+lead3`).  \n",
    "- **Winsorization**: clipped returns at 1stâ€“99th percentiles to reduce outlier influence.  \n",
    "- **Regression**: ran OLS  \n",
    "  \\[\n",
    "    \\text{return} = a + b \\cdot \\text{sentiment}\n",
    "  \\]  \n",
    "  and reported slope (Î²), t-stat, and sample size.  \n",
    "- **Plots**: two-panel figure showing sentiment vs 1D and 3D returns, each with scatter, OLS fit, and Î²/t-stat annotation.\n",
    "\n",
    "## Takeaway  \n",
    "This workflow connects **qualitative analyst sentiment** to **quantitative market reactions**, helping evaluate whether bullish/bearish analyst tone during calls aligns with **immediate reactions** or **post-call drift** in stock prices.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e01bfc-c311-46bc-a7dd-9a6d578f58cd",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "-- create analyst sentiment for Snowflake\n",
    "-- Note timestamp is in UTC\n",
    "WITH filtered_transcripts AS (\n",
    "    SELECT *\n",
    "    FROM SNOWFLAKE_PUBLIC_DATA_PAID.PUBLIC_DATA.COMPANY_EVENT_TRANSCRIPT_ATTRIBUTES\n",
    "    WHERE primary_ticker IN ('SNOW')\n",
    "-- start with Dow Jones 30 stocks \n",
    "      AND event_type = 'Earnings Call'\n",
    "      AND transcript_type = 'SPEAKERS_ANNOTATED'\n",
    "      AND transcript IS NOT NULL\n",
    "      AND event_timestamp >= '2024-01-01'\n",
    "    ORDER BY event_timestamp DESC\n",
    "),\n",
    "\n",
    "deduplicated_transcripts AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY primary_ticker, event_timestamp\n",
    "            ORDER BY created_at ASC   -- keep the earliest version - point-in-time \n",
    "        ) AS rn\n",
    "    FROM filtered_transcripts\n",
    "),\n",
    "\n",
    "unique_transcripts AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        transcript\n",
    "    FROM deduplicated_transcripts\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "ai_analysis AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        ai_complete(\n",
    "            'claude-4-sonnet', -- use claude for the moment \n",
    "            CONCAT_WS('\\n',\n",
    "            \n",
    "'You are analyzing sell side and buy side analysts sentiment in a public company''s earnings call transcript.',\n",
    "                \n",
    "'Focus ONLY on analyst questions, tone, and reactions to management responses.- Ignore managementâ€™s prepared remarks unless directly referenced by analysts. - Pay attention to how analysts compare results to prior earnings calls and to market expectations.',\n",
    "                \n",
    "'Use a CONSISTENT STANDARD across all earnings calls, regardless of sector, size, or company specifics.',\n",
    "                \n",
    "'Evaluate (internally, equal weight) the following aspects:',\n",
    "                \n",
    "'a) Guidance/Financial Outlook (revenue growth, earnings, margins, bookings, cash flow)',\n",
    "                \n",
    "'b) Strategy/Product/Innovation (new initiatives, products, technologies, or business models)',\n",
    "\n",
    "'c) Competitive/Market Positioning (market share, competition, regulation, macro/sector trends)',\n",
    "\n",
    "'d) Management Execution & Credibility (track record, transparency, quality of responses, consistency with prior guidance)',\n",
    "                \n",
    "'Compute ONE overall analyst sentiment score on a 1-10 scale, where:',\n",
    "                \n",
    "'1 = Extremely pessimistic/concerned, 5 = Neutral/mixed, 10 = Extremely optimistic/bullish.',\n",
    "                \n",
    "'Return your response as valid JSON with this exact format:',\n",
    "                \n",
    "'{\"score\": <integer 1-10>, \"reason\": <brief explanation covering the four aspects>, \"analyst_count\": <integer number of unique analysts>}',\n",
    "\n",
    "'Transcript:',\n",
    "\n",
    "transcript\n",
    "            )\n",
    "        ) AS ai_response\n",
    "    FROM unique_transcripts\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    primary_ticker,\n",
    "    event_timestamp,\n",
    "    event_type,\n",
    "    created_at,\n",
    "    /* Safely parse JSON. If parsing fails, these will be NULL. */\n",
    "    (TRY_PARSE_JSON(ai_response):score)::INT      AS sentiment_score,\n",
    "    (TRY_PARSE_JSON(ai_response):analyst_count)::INT AS unique_analyst_count,\n",
    "    (TRY_PARSE_JSON(ai_response):reason)::STRING  AS sentiment_reason\n",
    "\n",
    "    -- , ai_response AS raw_ai_response  -- Uncomment for debugging\n",
    "FROM ai_analysis\n",
    "where unique_analyst_count > 1 -- remove data that only contains speakers from the public company\n",
    "order by primary_ticker, event_timestamp desc; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e40a9-9968-49e0-9154-c791d43ad51b",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "-- create analyst sentiment for DOW JONES 30 company \n",
    "-- Note timestamp is in UTC\n",
    "WITH filtered_transcripts AS (\n",
    "    SELECT *\n",
    "    FROM SNOWFLAKE_PUBLIC_DATA_PAID.PUBLIC_DATA.COMPANY_EVENT_TRANSCRIPT_ATTRIBUTES\n",
    "    WHERE primary_ticker IN ('MMM', 'AXP', 'AMGN', 'AMZN', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'GS', 'HD', 'HON', 'IBM', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PG', 'RTX', 'CRM', 'SHW', 'TRV', 'UNH', 'V', 'WMT', 'NVDA')\n",
    "-- start with Dow Jones 30 stocks \n",
    "      AND event_type = 'Earnings Call'\n",
    "      AND transcript_type = 'SPEAKERS_ANNOTATED'\n",
    "      AND transcript IS NOT NULL\n",
    "      AND event_timestamp >= '2024-01-01'\n",
    "    ORDER BY event_timestamp DESC\n",
    "),\n",
    "\n",
    "deduplicated_transcripts AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY primary_ticker, event_timestamp\n",
    "            ORDER BY created_at ASC   -- keep the earliest version - point-in-time \n",
    "        ) AS rn\n",
    "    FROM filtered_transcripts\n",
    "),\n",
    "\n",
    "unique_transcripts AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        transcript\n",
    "    FROM deduplicated_transcripts\n",
    "    WHERE rn = 1\n",
    "),\n",
    "\n",
    "ai_analysis AS (\n",
    "    SELECT\n",
    "        primary_ticker,\n",
    "        event_timestamp,\n",
    "        event_type,\n",
    "        created_at,\n",
    "        ai_complete(\n",
    "            'claude-4-sonnet', -- use claude for the moment \n",
    "            CONCAT_WS('\\n',\n",
    "                'You are analyzing sell side and buy side analysts sentiment in a public company''s earnings call transcript.',\n",
    "                'Focus ONLY on analyst questions, tone, and reactions to management responses.- Ignore managementâ€™s prepared remarks unless directly referenced by analysts. - Pay attention to how analysts compare results to prior earnings calls and to market expectations.',\n",
    "                'Use a CONSISTENT STANDARD across all earnings calls, regardless of sector, size, or company specifics.',\n",
    "                'Evaluate (internally, equal weight) the following aspects:',\n",
    "                'a) Guidance/Financial Outlook (revenue growth, earnings, margins, bookings, cash flow)',\n",
    "                'b) Strategy/Product/Innovation (new initiatives, products, technologies, or business models)',\n",
    "                'c) Competitive/Market Positioning (market share, competition, regulation, macro/sector trends)',\n",
    "                'd) Management Execution & Credibility (track record, transparency, quality of responses, consistency with prior guidance)',\n",
    "                'Compute ONE overall analyst sentiment score on a 1-10 scale, where:',\n",
    "                '1 = Extremely pessimistic/concerned, 5 = Neutral/mixed, 10 = Extremely optimistic/bullish.',\n",
    "                'Return your response as valid JSON with this exact format:',\n",
    "                '{\"score\": <integer 1-10>, \"reason\": <brief explanation covering the four aspects>, \"analyst_count\": <integer number of unique analysts>}',\n",
    "                'Transcript:',\n",
    "                transcript\n",
    "            )\n",
    "        ) AS ai_response\n",
    "    FROM unique_transcripts\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    primary_ticker,\n",
    "    event_timestamp,\n",
    "    event_type,\n",
    "    created_at,\n",
    "    /* Safely parse JSON. If parsing fails, these will be NULL. */\n",
    "    (TRY_PARSE_JSON(ai_response):score)::INT      AS sentiment_score,\n",
    "    (TRY_PARSE_JSON(ai_response):analyst_count)::INT AS unique_analyst_count,\n",
    "    (TRY_PARSE_JSON(ai_response):reason)::STRING  AS sentiment_reason\n",
    "\n",
    "    -- , ai_response AS raw_ai_response  -- Uncomment for debugging\n",
    "FROM ai_analysis\n",
    "where unique_analyst_count > 1 -- remove data that only contains speakers from the public company\n",
    "order by primary_ticker, event_timestamp; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b230d-5a8c-4a21-8a4f-dab2d6af752b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "df_sentiment = cell4.to_pandas()\n",
    "df_sentiment.columns = df_sentiment.columns.str.lower()\n",
    "df_sentiment.rename(columns={'primary_ticker': 'ticker'}, inplace=True)\n",
    "df_sentiment['event_timestamp_et'] = (\n",
    "    pd.to_datetime(df_sentiment['event_timestamp'])\n",
    "    .dt.tz_localize('UTC')\n",
    "    .dt.tz_convert('US/Eastern')\n",
    ")\n",
    "print(df_sentiment[['ticker', 'event_timestamp', 'event_timestamp_et']].head(10))\n",
    "df_sentiment = df_sentiment.sort_values(['ticker', 'event_timestamp_et'])\n",
    "df_sentiment['sentiment_change'] = df_sentiment.groupby('ticker')['sentiment_score'].diff()\n",
    "df_sentiment['sentiment_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e0b52-b97e-4cd2-847a-cf8a3591b175",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "Read the price data, note we should use adjusted price instead of raw!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3c7ce-f8f9-4a49-9d8c-043c457c77dd",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM SNOWFLAKE_PUBLIC_DATA_PAID.PUBLIC_DATA.STOCK_PRICE_TIMESERIES \n",
    "WHERE ticker IN (\n",
    "    'MMM', 'AXP', 'AMGN', 'AMZN', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', \n",
    "    'DIS', 'GS', 'HD', 'HON', 'IBM', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', \n",
    "    'NKE', 'PG', 'RTX', 'CRM', 'SHW', 'TRV', 'UNH', 'V', 'WMT', 'NVDA'\n",
    ")\n",
    "  AND variable = 'post-market_close' -- make sure to use adjusted price later, waiting for the update from the data team\n",
    "ORDER BY ticker, date DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85872f-440d-4e38-b2b4-6b220f50754e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "df_price = cell11.to_pandas()\n",
    "df_price.columns = df_price.columns.str.lower()\n",
    "df_price.rename(columns={'value': 'price'}, inplace=True)\n",
    "## assume market close at 4:00 pm, again we should update with the right time once the data is ready  \n",
    "df_price['date'] = pd.to_datetime(df_price['date'])\n",
    "df_price['date_time'] = pd.to_datetime(df_price['date'].astype(str) + ' 16:00:00').dt.tz_localize('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bc88d-68fe-45a9-87b7-011d39d489f5",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# # Set up the plotting style\n",
    "# plt.style.use('default')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# # Get unique tickers (use 'primary_ticker' if you renamed the column)\n",
    "# ticker_list = df_price['ticker'].unique()  # or df_price['ticker'].unique()\n",
    "\n",
    "# # Create plots for each ticker\n",
    "# for ticker in ticker_list:\n",
    "#     # Filter data for current ticker\n",
    "#     ticker_data = df_price[df_price['ticker'] == ticker]\n",
    "#     ticker_data.sort_values('date_time')# or 'ticker'\n",
    "    \n",
    "#     # Create the plot\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(ticker_data['date_time'], ticker_data['price'], linewidth=2, marker='o', markersize=3)\n",
    "    \n",
    "#     # Customize the plot\n",
    "#     plt.title(f'{ticker} Stock Price Over Time', fontsize=16, fontweight='bold')\n",
    "#     plt.xlabel('Date', fontsize=12)\n",
    "#     plt.ylabel('Price ($)', fontsize=12)\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.xticks(rotation=45)\n",
    "    \n",
    "#     # Format y-axis as currency\n",
    "#     plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:.2f}'))\n",
    "    \n",
    "#     # Adjust layout and show\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98a8e2-8a6f-406b-804e-dbe83ec38b80",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "SPLIT_OR_ADJ_DATES = {\n",
    "    \"AAPL\": [\"2020-08-31\"],\n",
    "    \"AMZN\": [\"2022-06-06\"],\n",
    "    \"NVDA\": [\"2021-07-20\", \"2024-06-10\"],\n",
    "    \"WMT\":  [\"2024-02-26\"],\n",
    "    \"SHW\":  [\"2021-04-01\"],\n",
    "    \"HON\":  [\"2018-10-01\", \"2018-10-29\"],     # spin-off related adjustments\n",
    "    \"MRK\":  [\"2021-06-03\"],                    # Organon spin-off adjustment\n",
    "    \"MMM\":  [\"2024-04-01\"],                    # Solventum spin-off adjustment\n",
    "    \"IBM\":  [\"2021-11-04\"],                    # ex-distribution day for Kyndryl\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for tkr, dates in SPLIT_OR_ADJ_DATES.items():\n",
    "    for d in dates:\n",
    "        rows.append((tkr, d))\n",
    "\n",
    "df_split = pd.DataFrame(rows, columns=['ticker', 'date'])\n",
    "df_split['date'] = pd.to_datetime(df_split['date'])\n",
    "df_split['is_split'] = 1 \n",
    "df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ae4c9-2820-491d-8fd2-98a507a491f3",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "df_price = df_price.sort_values(['ticker', 'date'])\n",
    "# close to close return on day t\n",
    "df_price['return'] = df_price.groupby(['ticker'])['price'].transform(lambda x: np.log(x/x.shift(1)))\n",
    "df_price = df_price.merge(df_split, on = ['ticker', 'date'], how = 'left')\n",
    "df_price.loc[df_price['is_split'] == 1, 'return'] = 0 # mask out the days with stock split, change to adjusted stock split with better data \n",
    "df_price = df_price.sort_values(['ticker', 'date'])\n",
    "for i in range(1, 6):\n",
    "    df_price['return_lead_'+str(i)] = df_price.groupby(['ticker'])['return'].shift(-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bd85b-5cf5-486d-bcb4-aa9e8148f87d",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "Match the events with the market reaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c68f6-68e8-4114-9a57-b0dae8c81d1d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "def _to_naive_datetime(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if pd.api.types.is_datetime64tz_dtype(s.dtype):\n",
    "        # If tz-aware, drop timezone information safely\n",
    "        return s.dt.tz_convert(None)\n",
    "    else:\n",
    "        # If tz-naive, ensure it's explicitly naive\n",
    "        return s.dt.tz_localize(None)\n",
    "\n",
    "def next_price_after_event(df_sentiment, df_price,\n",
    "                           by=\"ticker\",\n",
    "                           event_col=\"event_timestamp_et\",\n",
    "                           price_time_col=\"date_time\"):\n",
    "    # 1) Clean & normalize\n",
    "    s = df_sentiment.dropna(subset=[by, event_col]).copy()\n",
    "    p = df_price.dropna(subset=[by, price_time_col]).copy()\n",
    "\n",
    "    s[by] = s[by].astype(str)\n",
    "    p[by] = p[by].astype(str)\n",
    "\n",
    "    s[event_col]     = _to_naive_datetime(s[event_col])\n",
    "    p[price_time_col]= _to_naive_datetime(p[price_time_col])\n",
    "\n",
    "    s = s.dropna(subset=[event_col])\n",
    "    p = p.dropna(subset=[price_time_col])\n",
    "\n",
    "    # 2) Global stable sort so each group's time is sorted\n",
    "    s = s.sort_values([by, event_col], kind=\"mergesort\")\n",
    "    p = p.sort_values([by, price_time_col], kind=\"mergesort\")\n",
    "\n",
    "    # 3) Per-ticker merge_asof (no dtype headaches, no renaming)\n",
    "    out = []\n",
    "    price_cols = [c for c in p.columns if c != by]  # keep ticker only from the left\n",
    "    for tkr, s_part in s.groupby(by, sort=False):\n",
    "        p_part = p[p[by] == tkr]\n",
    "        # Drop 'by' on the right to avoid duplicate ticker columns\n",
    "        m = pd.merge_asof(\n",
    "            s_part,\n",
    "            p_part[price_cols],\n",
    "            left_on=event_col,\n",
    "            right_on=price_time_col,\n",
    "            direction=\"forward\",\n",
    "            allow_exact_matches=False\n",
    "        )\n",
    "        out.append(m)\n",
    "\n",
    "    merged = pd.concat(out, ignore_index=True)\n",
    "\n",
    "    # 4) Report\n",
    "    n_events  = len(s)\n",
    "    n_matched = merged[price_time_col].notna().sum() if price_time_col in merged.columns else 0\n",
    "    print(f\"Successfully matched {n_matched} of {n_events} sentiment records\")\n",
    "    print(f\"Success rate: {100.0 * n_matched / n_events:.1f}%\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "# --- Use it ---\n",
    "merged_df = next_price_after_event(df_sentiment, df_price)\n",
    "print(\"\\nColumns:\", merged_df.columns.tolist())\n",
    "print(\"\\nHead:\\n\", merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31522d5f-564a-4162-a04b-0fbe1c17d179",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "Look at the 1 day stock reaction (during the call) and 3 day return after the call (post earning drift), winsorize the return reaction at 1 and 99 percentile\n",
    "\n",
    "\n",
    "Positive correlation between analyst sentiment and market reaction and drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb318f-4aff-4f01-a9b7-ff9e7da60c12",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Prep: 3-day return after the event (lead1+lead2+lead3)\n",
    "# ---------------------------------------------------------\n",
    "merged_df = merged_df.copy()\n",
    "merged_df['return_3d'] = merged_df[['return_lead_1', 'return_lead_2', 'return_lead_3']].sum(axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Winsorize (clip) helper at [1%, 99%]\n",
    "# ---------------------------------------------------------\n",
    "def winsorize_df(df, cols, lower=0.01, upper=0.99):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        q_low, q_hi = df[c].quantile([lower, upper])\n",
    "        df[c] = df[c].clip(lower=q_low, upper=q_hi)\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) OLS with t-stat for slope\n",
    "# ---------------------------------------------------------\n",
    "def ols_beta_t(x, y):\n",
    "    \"\"\"\n",
    "    Simple OLS: y ~ a + b*x\n",
    "    Returns: intercept a, slope b, t-stat of b, n\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x, y = x[m], y[m]\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan, np.nan, np.nan, n\n",
    "\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "    # OLS solution\n",
    "    beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)  # [a, b]\n",
    "    y_hat = X @ beta\n",
    "    resid = y - y_hat\n",
    "    s2 = (resid @ resid) / (n - 2)  # residual variance\n",
    "    XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    se_b = np.sqrt(s2 * XtX_inv[1, 1])\n",
    "    t_b = beta[1] / se_b if se_b > 0 else np.nan\n",
    "    return beta[0], beta[1], t_b, n\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Plot helper: scatter + OLS line + beta/t annotation\n",
    "# ---------------------------------------------------------\n",
    "def scatter_with_ols(ax, df, x_col, y_col, title, y_label, clip=True):\n",
    "    data = df[[x_col, y_col]].dropna().copy()\n",
    "    if clip:\n",
    "        data = winsorize_df(data, [y_col], 0.01, 0.99)\n",
    "\n",
    "    a, b, t, n = ols_beta_t(data[x_col], data[y_col])\n",
    "\n",
    "    ax.scatter(data[x_col], data[y_col], alpha=0.6, color = 'blue')\n",
    "\n",
    "    if np.isfinite(b):\n",
    "        x_vals = np.linspace(data[x_col].min(), data[x_col].max(), 200)\n",
    "        y_vals = a + b * x_vals\n",
    "        ax.plot(x_vals, y_vals, linewidth=2, label=\"OLS fit\", color = 'orange')\n",
    "\n",
    "    ax.set_xlabel(\"Sentiment Score\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    beta_str = f\"{b:.4f}\" if np.isfinite(b) else \"NA\"\n",
    "    t_str    = f\"{t:.2f}\" if np.isfinite(t) else \"NA\"\n",
    "    ax.text(0.02, 0.98, f\"Î² = {beta_str}\\nt-stat = {t_str}\\nn = {n}\",\n",
    "            transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.7, boxstyle=\"round\"))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Two-panel figure: 1D reaction and 3D drift\n",
    "# ---------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=False, sharey=False)\n",
    "\n",
    "# Left: 1-Day Return (initial reaction)\n",
    "scatter_with_ols(\n",
    "    axes[0],\n",
    "    merged_df,\n",
    "    x_col=\"sentiment_score\",\n",
    "    y_col=\"return\",\n",
    "    title=\"Analyst Sentiment and Daily Return During Earnings (OLS, 1â€“99% Winsorized)\",\n",
    "    y_label=\"1D Return\",\n",
    "    clip=True\n",
    ")\n",
    "\n",
    "# Right: 3-Day Return (post-call drift)\n",
    "scatter_with_ols(\n",
    "    axes[1],\n",
    "    merged_df,\n",
    "    x_col=\"sentiment_score\",\n",
    "    y_col=\"return_3d\",\n",
    "    title=\"Analyst Sentiment and 3D Return after Earnings (OLS, 1â€“99% Winsorized)\",\n",
    "    y_label=\"3D Return\",\n",
    "    clip=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97a55f-a9e1-4d76-9e51-616af6cf3dd1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# use the change of analyst sentiment (vs last earning call)\n",
    "# higher analyst sentiment (compared to last earnings call) -> higher stock return\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=False, sharey=False)\n",
    "\n",
    "# Left: 1-Day Return (initial reaction)\n",
    "scatter_with_ols(\n",
    "    axes[0],\n",
    "    merged_df,\n",
    "    x_col=\"sentiment_change\",\n",
    "    y_col=\"return\",\n",
    "    title=\"Sentiment(vs last earnings call) and Daily Return During Earnings (1â€“99% Winsorized)\",\n",
    "    y_label=\"1D Return\",\n",
    "    clip=True\n",
    ")\n",
    "\n",
    "# Right: 3-Day Return (post-call drift)\n",
    "scatter_with_ols(\n",
    "    axes[1],\n",
    "    merged_df,\n",
    "    x_col=\"sentiment_change\",\n",
    "    y_col=\"return_3d\",\n",
    "    title=\"Sentiment(vs last earnings call) and 3D Return after Earnings (1â€“99% Winsorized)\",\n",
    "    y_label=\"3D Return\",\n",
    "    clip=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633cffe-f99a-4771-9f90-3e553bc7bcf5",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "# ML Model for Financial Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e10a5-2105-4294-a7cb-bc3ef331ed7c",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "## ðŸ“Œ Goal  \n",
    "Test whether short-, medium-, and long-horizon return **slices** help **predict 5-day forward log returns** (with a 1-day implementation lag) using LightGBM, then deploy those forecasts in a simple **weekly long/short** strategy.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¨ What We Did  \n",
    "\n",
    "1. **Feature Engineering (no leakage)**  \n",
    "   - Compute daily **log returns** and **clip** at the 1st/99th percentiles (applied once so X and y share identical preprocessing).  \n",
    "   - Build **non-overlapping momentum slices** ending at day *t*:  \n",
    "     - `r_1` (today)  \n",
    "     - `r_5_1` (t-4..t-1)  \n",
    "     - `r_10_5` (t-9..t-5)  \n",
    "     - `r_21_10` (t-20..t-11)  \n",
    "     - `r_63_21` (t-62..t-21)  \n",
    "\n",
    "2. **Label Construction (live-consistent)**  \n",
    "   - Target **y(t) = Î£ logret[t+2..t+6]**.  \n",
    "   - Interpretation: predict at **close t**, trade at **close t+1** (1-day lag), and realize from **t+2 through t+6**.  \n",
    "\n",
    "3. **Walk-Forward Training (quarterly)**  \n",
    "   - For each test quarter:  \n",
    "     - **Train** on all quarters \\< (Q-2)  \n",
    "     - **Validate** on (Q-2, Q-1)  \n",
    "     - **Test** on **Q**  \n",
    "   - Enforce **strict cutoffs** so rows needing returns beyond the split end are dropped (**no look-ahead**).  \n",
    "\n",
    "4. **Modeling (LightGBM)**  \n",
    "   - LightGBM **regression** with early stopping.  \n",
    "   - **Small grid search** each quarter over `learning_rate` and `num_leaves` for a quick biasâ€“variance sweep.  \n",
    "\n",
    "5. **Prediction & Portfolio (Tue â†’ Wed)**  \n",
    "   - Generate forecasts on **Tuesdays** in the test quarter.  \n",
    "   - At **Wednesday close**, go **long top-5** and **short bottom-5** by predicted return (equal weight).  \n",
    "   - Hold through **Thu â†’ next Wed** (exactly the `t+2..t+6` window used for training).  \n",
    "   - **Transaction cost**: 3.0 bps one-way applied via weekly turnover.  \n",
    "\n",
    "6. **Performance Evaluation**  \n",
    "   - **Forecast quality**: RMSE (and optional RÂ²) on Tuesday predictions.  \n",
    "   - **Strategy metrics**:  \n",
    "     - Information Ratio (before/after costs)  \n",
    "     - Turnover & cost impact (bps)  \n",
    "     - Max drawdown & Calmar  \n",
    "     - Equity curves (before vs after cost)  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Takeaways  \n",
    "- The pipeline cleanly separates **signal quality** from **execution realism** (turnover, costs, drawdowns).  \n",
    "- Quarterly re-training helps LightGBM **adapt to regime shifts**.  \n",
    "- Even with compact features and a tiny grid, forecasts can be usefulâ€”yet **costs matter**: at 3 bps one-way, turnover meaningfully affects realized alpha.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ **In short:** an **end-to-end, no-leakage backtest** where LightGBM predicts near-term returns from orthogonalized momentum slices, converted into a **systematic weekly L/S portfolio** with explicit timing (Tue signal â†’ Wed trade) and realistic cost modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c5662-db88-40f5-8734-ac5e4ccfe6cf",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# End-to-end LightGBM demo for weekly L/S portfolio on stocks\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build *log* returns, globally clip at 1%/99% once (X & y consistent)\n",
    "# 2) Features (non-overlapping \"slices\" of past returns ending at t):\n",
    "#       r_1, r_5_1, r_10_5, r_21_10, r_63_21\n",
    "# 3) Label y(t) = sum of log returns t+2..t+6 (lag=1 day, 5-day horizon)\n",
    "# 4) Train quarterly: TRAIN (<Q-2), VALID (Q-2,Q-1), TEST (Q)\n",
    "#    Predict only on Tuesdays of test quarter (signals at Tue close)\n",
    "# 5) Portfolio: form at Wed close (one-day lag), hold Thuâ†’next Wed\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# -----------------------------\n",
    "# Config â€” horizons & portfolio\n",
    "# -----------------------------\n",
    "LAG_DAYS     = 2   # start at t+2 because we implement at t+1 (one-day lag)\n",
    "HORIZON_DAYS = 5   # total of 5 trading days: t+2..t+6\n",
    "TOP_K        = 5\n",
    "BOT_K        = 5\n",
    "MIN_XS       = 10  # min names on a Tuesday to form portfolio\n",
    "COST_BPS     = 3.0 # one-way cost per unit turnover (bps)\n",
    "\n",
    "# ------------------------------------\n",
    "# 0) Utilities: dates & simple metrics\n",
    "# ------------------------------------\n",
    "def to_quarter(dates):\n",
    "    return pd.PeriodIndex(pd.to_datetime(dates), freq='Q')\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def _build_bd_shift(trading_dates):\n",
    "    \"\"\"Business-day shifter on your union of trading dates.\"\"\"\n",
    "    trading_dates = pd.Index(pd.to_datetime(np.sort(pd.unique(trading_dates))))\n",
    "    pos = pd.Series(np.arange(len(trading_dates)), index=trading_dates)\n",
    "    def bd_shift(date, n):\n",
    "        if pd.isna(date): return pd.NaT\n",
    "        i = pos.get(date, None)\n",
    "        if i is None: return pd.NaT\n",
    "        j = i + n\n",
    "        return trading_dates[j] if 0 <= j < len(trading_dates) else pd.NaT\n",
    "    return bd_shift, trading_dates\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Returns prep: log returns + one-time (global) clipping\n",
    "#    Why once? Keeps preprocessing identical for X and y.\n",
    "# ------------------------------------------------------------\n",
    "def ensure_log_returns_and_clip(df, q_low=0.01, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Input must contain: 'date','ticker', and 'close' (or 'price').\n",
    "    Output adds 'return' = log(P_t/P_{t-1}) and clips it globally at\n",
    "    [q_low, q_high] percentiles using the *full* sample.\n",
    "    \"\"\"\n",
    "    need = {'date','ticker'}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(\"df must have ['date','ticker', plus 'close' or 'price'].\")\n",
    "\n",
    "    px = 'close' if 'close' in df.columns else ('price' if 'price' in df.columns else None)\n",
    "    if px is None:\n",
    "        raise ValueError(\"Provide a price column named 'close' (preferred) or 'price'.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['ticker','date'])\n",
    "\n",
    "    # log return per ticker\n",
    "    df['return'] = df.groupby('ticker', group_keys=False)[px].apply(\n",
    "        lambda s: np.log(s) - np.log(s.shift(1))\n",
    "    )\n",
    "\n",
    "    # one-time global clipping (FULL sample, this is for simplicity you should do it using a rolling window in practice)\n",
    "    lo, hi = df['return'].quantile([q_low, q_high])\n",
    "    df['return'] = df['return'].clip(lower=lo, upper=hi)\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2) Features & label\n",
    "#    Features at close t use info up to & including t.\n",
    "#    Label y(t) = sum of clipped log returns t+2..t+6 (default).\n",
    "# ----------------------------------------------------------------\n",
    "def make_fullpanel_features_labels(df,\n",
    "                                   lag_days=LAG_DAYS,\n",
    "                                   horizon_days=HORIZON_DAYS):\n",
    "    df = ensure_log_returns_and_clip(df)\n",
    "    g  = df.groupby('ticker', group_keys=False)\n",
    "\n",
    "    # rolling sums ending at t (include t)\n",
    "    r   = g['return']\n",
    "    r5  = r.transform(lambda s: s.rolling(5).sum())\n",
    "    r10 = r.transform(lambda s: s.rolling(10).sum())\n",
    "    r21 = r.transform(lambda s: s.rolling(21).sum())\n",
    "    r63 = r.transform(lambda s: s.rolling(63).sum())\n",
    "\n",
    "    # orthogonalized slices (non-overlapping)\n",
    "    df['r_1']      = r.transform(lambda s: s)          # today\n",
    "    df['r_5_1']    = r5  - df['r_1']                   # days t-4..t-1\n",
    "    df['r_10_5']   = r10 - r5                          # days t-9..t-5\n",
    "    df['r_21_10']  = r21 - r10                         # days t-20..t-11\n",
    "    df['r_63_21']  = r63 - r21                         # days t-62..t-21\n",
    "    feat_cols = ['r_1','r_5_1','r_10_5','r_21_10','r_63_21']\n",
    "\n",
    "    # label y(t): forward window starting at t+lag_days; length = horizon_days\n",
    "    # We align the *end* of the window to t by shifting forward, then rolling.\n",
    "    def fwd_sum_label(s):\n",
    "        return s.shift(-(lag_days + horizon_days - 1)).rolling(horizon_days).sum()\n",
    "    df['y'] = r.transform(fwd_sum_label)\n",
    "\n",
    "    realized_col = f'realized_tplus{lag_days}_x{horizon_days}'\n",
    "    df[realized_col] = df['y']\n",
    "\n",
    "    # drop rows without complete features/labels\n",
    "    df = df.dropna(subset=feat_cols + ['y']).reset_index(drop=True)\n",
    "    \n",
    "    return df, feat_cols, realized_col\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Model: tiny LightGBM grid + early stopping\n",
    "# ------------------------------------------------------------\n",
    "def make_param_grid(base=None):\n",
    "    base = base or {}\n",
    "    grid = {\n",
    "        \"learning_rate\": [0.03, 0.05, 0.10],\n",
    "        \"num_leaves\":    [31, 63],\n",
    "    }\n",
    "    fixed = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"l2\",\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"feature_fraction\": 0.75,\n",
    "        \"bagging_fraction\": 0.75,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"lambda_l2\": 1.0,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": base.get(\"seed\", 42),\n",
    "    }\n",
    "    out = []\n",
    "    for lr in grid[\"learning_rate\"]:\n",
    "        for nl in grid[\"num_leaves\"]:\n",
    "            p = fixed.copy()\n",
    "            p.update({\"learning_rate\": lr, \"num_leaves\": nl})\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def train_with_hp_search(Xtr, ytr, Xva, yva,\n",
    "                         base_params=None, num_boost_round=5000,\n",
    "                         es_rounds=200, verbose=False):\n",
    "    grid  = make_param_grid(base_params)\n",
    "    best  = {\"score\": float(\"inf\"), \"model\": None, \"params\": None}\n",
    "    dtr   = lgb.Dataset(Xtr, label=ytr, free_raw_data=False)\n",
    "    dva   = lgb.Dataset(Xva, label=yva, free_raw_data=False)\n",
    "\n",
    "    for i, params in enumerate(grid, 1):\n",
    "        model = lgb.train(\n",
    "            params, dtr, num_boost_round=num_boost_round,\n",
    "            valid_sets=[dva], valid_names=[\"valid\"],\n",
    "            callbacks=[early_stopping(es_rounds), log_evaluation(0 if not verbose else 50)]\n",
    "        )\n",
    "        score = model.best_score.get(\"valid\", {}).get(\"l2\", float(\"inf\"))\n",
    "        if verbose:\n",
    "            print(f\"[{i:>2}/{len(grid)}] valid_l2={score:.6f}  \"\n",
    "                  f\"lr={params['learning_rate']}  leaves={params['num_leaves']}\")\n",
    "        if score < best[\"score\"]:\n",
    "            best.update({\"score\": score, \"model\": model, \"params\": params})\n",
    "    return best[\"model\"], best[\"params\"], best[\"score\"]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Quarterly loop: predict *Tuesdays* in test quarter (strict cutoff)\n",
    "#     - TRAIN = all quarters before the last two (Q-2, Q-1)\n",
    "#     - VALID = Q-2 and Q-1\n",
    "#     - TEST  = Q\n",
    "#   Strictly enforce: keep only rows with t â‰¤ (split_end - (lag+horizon-1)bd)\n",
    "# -------------------------------------------------------------------\n",
    "def quarterly_train_predict_tuesdays(df_price,\n",
    "                                     lag_days=LAG_DAYS,\n",
    "                                     horizon_days=HORIZON_DAYS,\n",
    "                                     params=None,\n",
    "                                     num_boost_round=5000,\n",
    "                                     es_rounds=200,\n",
    "                                     random_state=42,\n",
    "                                     log_model=True):\n",
    "    if params is None:\n",
    "        params = dict(\n",
    "            objective='regression', metric='l2',\n",
    "            learning_rate=0.05, num_leaves=63,\n",
    "            min_data_in_leaf=50, feature_fraction=0.9,\n",
    "            bagging_fraction=0.8, bagging_freq=1,\n",
    "            lambda_l2=1.0, verbosity=-1, seed=random_state\n",
    "        )\n",
    "\n",
    "    panel, feat_cols, realized_col = make_fullpanel_features_labels(\n",
    "        df_price, lag_days=lag_days, horizon_days=horizon_days\n",
    "    )\n",
    "    panel['q'] = to_quarter(panel['date'])\n",
    "\n",
    "    bd_shift, _ = _build_bd_shift(panel['date'])\n",
    "    quarters    = sorted(panel['q'].unique())\n",
    "    lookahead   = lag_days + horizon_days - 1   # = 6 with defaults\n",
    "\n",
    "    preds_list = []\n",
    "\n",
    "    # Need â‰¥ 4 train + 2 val + 1 test quarters â†’ start at index 6\n",
    "    for i in range(6, len(quarters)):\n",
    "        q_test  = quarters[i]\n",
    "        q_val   = quarters[i-2:i]\n",
    "        q_train = quarters[:i-2]\n",
    "        if len(q_train) < 4:\n",
    "            continue\n",
    "\n",
    "        train_all = panel[panel['q'].isin(q_train)].copy()\n",
    "        val_all   = panel[panel['q'].isin(q_val)].copy()\n",
    "        testQ     = panel[panel['q'] == q_test].copy()\n",
    "        if train_all.empty or val_all.empty or testQ.empty:\n",
    "            continue\n",
    "\n",
    "        # strict cutoffs to avoid label peeking past split ends\n",
    "        cutoff_train = bd_shift(train_all['date'].max(), -lookahead)\n",
    "        cutoff_val   = bd_shift(val_all['date'].max(),   -lookahead)\n",
    "        if pd.isna(cutoff_train) or pd.isna(cutoff_val):\n",
    "            continue\n",
    "\n",
    "        train = train_all[train_all['date'] <= cutoff_train].copy()\n",
    "        valid = val_all[val_all['date']   <= cutoff_val].copy()\n",
    "        if train.empty or valid.empty:\n",
    "            continue\n",
    "\n",
    "        # (No extra winsorization â€” we already globally clipped returns)\n",
    "        Xtr, ytr = train[feat_cols], train['y']\n",
    "        Xva, yva = valid[feat_cols], valid['y']\n",
    "\n",
    "        model, best_params, best_score = train_with_hp_search(\n",
    "            Xtr, ytr, Xva, yva,\n",
    "            base_params=params,\n",
    "            num_boost_round=num_boost_round,\n",
    "            es_rounds=es_rounds,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if log_model:\n",
    "            # Register best model for the quarter (Dash)\n",
    "            model_name = f\"FIS_{q_test}\"\n",
    "            registry.log_model(model,\n",
    "                            model_name=model_name,\n",
    "                            version_name=\"v1\",\n",
    "                            sample_input_data=Xtr.head(100),\n",
    "                            options={\n",
    "                                \"relax_version\": False,\n",
    "                                \"target_methods\": [\"predict\"],\n",
    "                                \"method_options\": {\n",
    "                                    \"predict\": {\"case_sensitive\": True}\n",
    "                                }\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        # Predict only on Tuesdays in TEST quarter\n",
    "        test_tues = testQ[testQ['date'].dt.weekday == 1]  # Monday=0, Tuesday=1\n",
    "        if test_tues.empty:\n",
    "            continue\n",
    "\n",
    "        X_test = test_tues[feat_cols]\n",
    "        yhat   = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        out = test_tues[['date','ticker','y']].copy()\n",
    "        out['y_hat'] = yhat\n",
    "        out.rename(columns={'date':'signal_date'}, inplace=True)\n",
    "\n",
    "        # Attach trade date (Wednesday) and holding end (next Wednesday)\n",
    "        out['trade_date'] = out['signal_date'].apply(lambda d: bd_shift(d, 1))  # Wed\n",
    "        out['period_end'] = out['signal_date'].apply(lambda d: bd_shift(d, 6))  # next Wed\n",
    "        out['week_key']   = out['period_end']  # key for weekly PnL aggregation\n",
    "        preds_list.append(out)\n",
    "\n",
    "    if not preds_list:\n",
    "        raise ValueError(\"No eligible test quarters or Tuesday signals after strict cutoffs.\")\n",
    "    preds = pd.concat(preds_list, ignore_index=True).sort_values(['signal_date','ticker'])\n",
    "    return preds, realized_col, feat_cols\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Portfolio: Tue signal â†’ trade Wed â†’ realize Thu..next Wed (y)\n",
    "# ------------------------------------------------------------------\n",
    "def weekly_ls_portfolio_tue_wed(preds, top_k=TOP_K, bottom_k=BOT_K,\n",
    "                                min_names=MIN_XS, cost_per_turn_bps=COST_BPS):\n",
    "    \"\"\"\n",
    "    - On each Tuesday, rank by y_hat; long top_k, short bottom_k (equal-weight).\n",
    "    - Form at Wednesday close; realized PnL for the \"week\" is y(signal_date).\n",
    "    - Only rebalance on Wednesdays; turnover vs last Wednesday.\n",
    "    \"\"\"\n",
    "    sig = preds.copy()\n",
    "\n",
    "    picks = []\n",
    "    for t_sig, g in sig.groupby('signal_date', sort=True):\n",
    "        if len(g) < min_names:\n",
    "            continue\n",
    "        g = g.sort_values('y_hat')\n",
    "        k_top = min(top_k, len(g))\n",
    "        k_bot = min(bottom_k, len(g))\n",
    "        longs  = g.tail(k_top).assign(side=+1.0)\n",
    "        shorts = g.head(k_bot).assign(side=-1.0)\n",
    "        picks.append(pd.concat([longs, shorts], axis=0))\n",
    "\n",
    "    if not picks:\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    picks = pd.concat(picks, ignore_index=True)\n",
    "\n",
    "    weekly_rows = []\n",
    "    prev_w = pd.Series(dtype=float)\n",
    "\n",
    "    for week_end, d in picks.groupby('period_end', sort=True):\n",
    "        # realized next-Wed PnL for holdings selected on prior Tuesday\n",
    "        r = d.set_index('ticker')['y'].astype(float).dropna()\n",
    "\n",
    "        long_tickers  = d.loc[d['side'] > 0, 'ticker'].tolist()\n",
    "        short_tickers = d.loc[d['side'] < 0, 'ticker'].tolist()\n",
    "        nL, nS = len(long_tickers), len(short_tickers)\n",
    "        if nL == 0 or nS == 0 or r.empty:\n",
    "            continue\n",
    "\n",
    "        # equal-weight L/S\n",
    "        w = pd.Series(0.0, index=pd.Index(long_tickers + short_tickers))\n",
    "        w.loc[long_tickers]  =  1.0 / nL\n",
    "        w.loc[short_tickers] = -1.0 / nS\n",
    "\n",
    "        # before-cost weekly PnL (using *log-return sums*)\n",
    "        ret_before = (w.reindex(r.index).fillna(0.0) * r).sum()\n",
    "\n",
    "        # turnover vs last Wed\n",
    "        union  = w.index.union(prev_w.index)\n",
    "        w_u    = w.reindex(union).fillna(0.0)\n",
    "        prev_u = prev_w.reindex(union).fillna(0.0)\n",
    "        turnover = 0.5 * (w_u - prev_u).abs().sum()\n",
    "\n",
    "        cost = (cost_per_turn_bps / 1e4) * turnover\n",
    "        ret_after = ret_before - cost\n",
    "\n",
    "        weekly_rows.append({\n",
    "            'week_end': week_end,\n",
    "            'ret_before_cost': ret_before,\n",
    "            'ret_after_cost':  ret_after,\n",
    "            'turnover': turnover\n",
    "        })\n",
    "        prev_w = w\n",
    "\n",
    "    weekly = pd.DataFrame(weekly_rows).sort_values('week_end').reset_index(drop=True)\n",
    "    if weekly.empty:\n",
    "        return weekly, {}\n",
    "\n",
    "    # equity & drawdown\n",
    "    weekly['equity_before'] = (1 + weekly['ret_before_cost']).cumprod()\n",
    "    weekly['equity_after']  = (1 + weekly['ret_after_cost']).cumprod()\n",
    "    peak = weekly['equity_before'].cummax()\n",
    "    weekly['drawdown'] = weekly['equity_before'] / peak - 1.0\n",
    "\n",
    "    # summary stats (annualize by 52 weeks)\n",
    "    mu_b, sd_b = weekly['ret_before_cost'].mean(), weekly['ret_before_cost'].std(ddof=1)\n",
    "    mu_a, sd_a = weekly['ret_after_cost'].mean(),  weekly['ret_after_cost'].std(ddof=1)\n",
    "    ir_before  = (mu_b/sd_b)*np.sqrt(52) if sd_b>0 else np.nan\n",
    "    ir_after   = (mu_a/sd_a)*np.sqrt(52) if sd_a>0 else np.nan\n",
    "    mdd_before = weekly['drawdown'].min()\n",
    "    ann_b      = mu_b * 52\n",
    "    calmar_b   = (ann_b / abs(mdd_before)) if (mdd_before < 0) else np.nan\n",
    "\n",
    "    stats = dict(\n",
    "        IR_before=ir_before, IR_after=ir_after,\n",
    "        max_drawdown_before=mdd_before, calmar_before=calmar_b,\n",
    "        avg_turnover=weekly['turnover'].mean(),\n",
    "        p90_turnover=weekly['turnover'].quantile(0.9),\n",
    "        cost_bps=cost_per_turn_bps\n",
    "    )\n",
    "    return weekly, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7ef92-e80e-4573-bf6c-aee16466522f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Run the demo\n",
    "# =========================\n",
    "# Expect df_price with columns: ['date','ticker','close']  (or 'price')\n",
    "# df_price = ... (load your panel here)\n",
    "\n",
    "tue_preds, realized_col, feat_cols = quarterly_train_predict_tuesdays(\n",
    "    df_price,\n",
    "    lag_days=LAG_DAYS, horizon_days=HORIZON_DAYS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b2ad7-ef72-46e0-8bb2-845c567119fb",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "Interpretation: Even a simple model applied to Dow Jones component stocks can deliver a reasonable PnL, achieving an IR of around 0.5 (before cost). \n",
    "\n",
    "\n",
    "Potential improvements: \\\n",
    "Incorporate risk-adjusted returns for training and portfolio construction (e.g., residuals after controlling for market and/or industry exposures). \\\n",
    "Add more predictive signals and expand the investment universe to increase diversification and alpha potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370109a5-eefc-4596-922b-5ca0e0f1005b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "weekly_pnl, stats = weekly_ls_portfolio_tue_wed(\n",
    "    tue_preds,\n",
    "    top_k=TOP_K, bottom_k=BOT_K, min_names=MIN_XS, cost_per_turn_bps=COST_BPS\n",
    ")\n",
    "\n",
    "if weekly_pnl.empty or not stats:\n",
    "    print(\"No weekly PnL generated. Check Tuesday coverage, MIN_XS, TOP/BOT K, and lookahead cutoffs.\")\n",
    "else:\n",
    "    print(f\"IR before cost : {stats['IR_before']:.3f}\")\n",
    "    print(f\"IR after  cost : {stats['IR_after']:.3f}\")\n",
    "    print(f\"Max DD (before): {stats['max_drawdown_before']:.2%}\")\n",
    "    print(f\"Calmar (before): {stats['calmar_before']:.3f}\")\n",
    "    print(f\"Avg turnover   : {stats['avg_turnover']:.3f}\")\n",
    "\n",
    "    # PnL curves\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(weekly_pnl['week_end'], weekly_pnl['equity_before'], label='Before cost')\n",
    "    plt.plot(weekly_pnl['week_end'], weekly_pnl['equity_after'],  label='After cost')\n",
    "    plt.xlabel('Week end (Wed)')\n",
    "    plt.ylabel('Equity (compounded)')\n",
    "    plt.title('Cumulative PnL: Before vs After Costs (Tue signal â†’ Wed trade)')\n",
    "    plt.legend(); plt.grid(alpha=0.3); plt.show()\n",
    "\n",
    "    # Drawdown (before cost only)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(weekly_pnl['week_end'], weekly_pnl['drawdown'])\n",
    "    plt.xlabel('Week end (Wed)')\n",
    "    plt.ylabel('Drawdown')\n",
    "    plt.title('Drawdown Curve (Before Cost)')\n",
    "    plt.grid(alpha=0.3); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "jonathan.regenstein@snowflake.com",
   "authorId": "4207858164052",
   "authorName": "JREGENSTEIN",
   "lastEditTime": 1758936580114,
   "notebookId": "m6i3mytkykwqhkxhuh7g",
   "sessionId": "b2d9e00f-beff-4a8f-88c8-a99f6769046b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

#!/bin/bash
#
# FSI Cortex Assistant - SnowCLI Deployment Script
# 
# Usage: ./deploy.sh
#
# Requirements:
# - SnowCLI installed (pip install snowflake-cli-labs)
# - Snowflake account with ACCOUNTADMIN access
# - SnowCLI configured with connection profile
#

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DATA_DIR="$SCRIPT_DIR/dataops/event/DATA"
SQL_DIR="$SCRIPT_DIR/dataops/event"
DEPLOY_DIR="$SCRIPT_DIR/deployment"

# Snowflake Configuration (can be overridden with environment variables)
export SNOWFLAKE_DATABASE="${SNOWFLAKE_DATABASE:-ACCELERATE_AI_IN_FSI}"
export SNOWFLAKE_SCHEMA="${SNOWFLAKE_SCHEMA:-DEFAULT_SCHEMA}"
export SNOWFLAKE_WAREHOUSE="${SNOWFLAKE_WAREHOUSE:-DEFAULT_WH}"
export SNOWFLAKE_ROLE="${SNOWFLAKE_ROLE:-ATTENDEE_ROLE}"
export DOCUMENT_AI_SCHEMA="${DOCUMENT_AI_SCHEMA:-DOCUMENT_AI}"
export CORTEX_ANALYST_SCHEMA="${CORTEX_ANALYST_SCHEMA:-CORTEX_ANALYST}"
export NOTEBOOKS_SCHEMA="${NOTEBOOKS_SCHEMA:-NOTEBOOKS}"
export STREAMLIT_SCHEMA="${STREAMLIT_SCHEMA:-STREAMLIT}"
export AISQL_EXAMPLES="${AISQL_EXAMPLES:-AISQL_EXAMPLES}"
export HACKATHON_DATABASE="${HACKATHON_DATABASE:-HACKATHON_DATASETS}"

echo -e "${BLUE}╔══════════════════════════════════════════════════════════════╗${NC}"
echo -e "${BLUE}║  FSI Cortex Assistant - SnowCLI Deployment                   ║${NC}"
echo -e "${BLUE}╚══════════════════════════════════════════════════════════════╝${NC}"
echo ""

# Check prerequisites
echo -e "${YELLOW}[1/10] Checking prerequisites...${NC}"

if ! command -v snow &> /dev/null; then
    echo -e "${RED}ERROR: SnowCLI not found. Install with: pip install snowflake-cli-labs${NC}"
    exit 1
fi

if ! command -v python3 &> /dev/null; then
    echo -e "${RED}ERROR: Python 3 not found. Please install Python 3.8 or later${NC}"
    exit 1
fi

echo -e "${GREEN}✓ Prerequisites check passed${NC}"
echo ""

# Create deployment directory
mkdir -p "$DEPLOY_DIR"

# Generate deployment SQL files
echo -e "${YELLOW}[2/10] Generating deployment SQL files...${NC}"

cat > "$DEPLOY_DIR/00_config.sql" << 'EOF'
-- FSI Cortex Assistant Configuration
-- Generated by deploy.sh

-- Set session parameters
ALTER SESSION SET QUERY_TAG = '{"origin":"sf_quickstart", "name":"Build an AI Assistant for FSI", "version":"2.0"}';

-- Configuration variables (edit these if needed)
SET DATABASE_NAME = 'ACCELERATE_AI_IN_FSI';
SET WAREHOUSE_NAME = 'DEFAULT_WH';
SET ATTENDEE_ROLE_NAME = 'ATTENDEE_ROLE';

EOF

echo -e "${GREEN}✓ Config generated${NC}"

# Step 1: Account Setup
echo -e "${YELLOW}[3/10] Creating account setup script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
doc_ai_schema = os.environ.get('DOCUMENT_AI_SCHEMA')
cortex_schema = os.environ.get('CORTEX_ANALYST_SCHEMA')
notebooks_schema = os.environ.get('NOTEBOOKS_SCHEMA')
streamlit_schema = os.environ.get('STREAMLIT_SCHEMA')

# Read configure_attendee_account.template.sql
with open(f'{sql_dir}/configure_attendee_account.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.DOCUMENT_AI_SCHEMA \}\}': doc_ai_schema,
    r'\{\{ env\.CORTEX_ANALYST_SCHEMA \}\}': cortex_schema,
    r'\{\{ env\.NOTEBOOKS_SCHEMA \}\}': notebooks_schema,
    r'\{\{ env\.STREAMLIT_SCHEMA \}\}': streamlit_schema,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/01_configure_account.sql', 'w') as f:
    f.write(content)

print("✓ Account setup script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Account setup generated${NC}"

# Step 2: Data Foundation
echo -e "${YELLOW}[4/10] Creating data foundation script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
doc_ai_schema = os.environ.get('DOCUMENT_AI_SCHEMA')
cortex_schema = os.environ.get('CORTEX_ANALYST_SCHEMA')

# Read data_foundation.template.sql
with open(f'{sql_dir}/data_foundation.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.DOCUMENT_AI_SCHEMA \}\}': doc_ai_schema,
    r'\{\{ env\.CORTEX_ANALYST_SCHEMA \}\}': cortex_schema,
    r'\{\{ env\.CI_PROJECT_DIR \}\}': script_dir,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/02_data_foundation.sql', 'w') as f:
    f.write(content)

print("✓ Data foundation script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Data foundation generated${NC}"

# Step 3: Cortex Analyst
echo -e "${YELLOW}[5/10] Creating Cortex Analyst deployment script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
doc_ai_schema = os.environ.get('DOCUMENT_AI_SCHEMA')
cortex_schema = os.environ.get('CORTEX_ANALYST_SCHEMA')

# Read deploy_cortex_analyst.template.sql
with open(f'{sql_dir}/deploy_cortex_analyst.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.DOCUMENT_AI_SCHEMA \}\}': doc_ai_schema,
    r'\{\{ env\.CORTEX_ANALYST_SCHEMA \}\}': cortex_schema,
    r'\{\{ env\.CI_PROJECT_DIR \}\}': script_dir,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/03_deploy_cortex_analyst.sql', 'w') as f:
    f.write(content)

print("✓ Cortex Analyst deployment script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Cortex Analyst generated${NC}"

# Step 4: Streamlit
echo -e "${YELLOW}[6/10] Creating Streamlit deployment script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
streamlit_schema = os.environ.get('STREAMLIT_SCHEMA')

# Read deploy_streamlit.template.sql
with open(f'{sql_dir}/deploy_streamlit.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.STREAMLIT_SCHEMA \}\}': streamlit_schema,
    r'\{\{ env\.CORTEX_ANALYST_SCHEMA \}\}': os.environ.get('CORTEX_ANALYST_SCHEMA'),
    r'\{\{ env\.CI_PROJECT_DIR \}\}': script_dir,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/04_deploy_streamlit.sql', 'w') as f:
    f.write(content)

print("✓ Streamlit deployment script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Streamlit deployment generated${NC}"

# Step 5: Notebooks
echo -e "${YELLOW}[7/10] Creating notebooks deployment script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
notebooks_schema = os.environ.get('NOTEBOOKS_SCHEMA')

# Read deploy_notebooks.template.sql
with open(f'{sql_dir}/deploy_notebooks.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.NOTEBOOKS_SCHEMA \}\}': notebooks_schema,
    r'\{\{ env\.CI_PROJECT_DIR \}\}': script_dir,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/05_deploy_notebooks.sql', 'w') as f:
    f.write(content)

print("✓ Notebooks deployment script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Notebooks deployment generated${NC}"

# Step 6: AISQL Examples
echo -e "${YELLOW}[8/10] Creating AISQL examples configuration script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
aisql_examples = os.environ.get('AISQL_EXAMPLES')
notebooks_schema = os.environ.get('NOTEBOOKS_SCHEMA')

# Read configure_aisql_examples.template.sql
with open(f'{sql_dir}/configure_aisql_examples.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.AISQL_EXAMPLES \}\}': aisql_examples,
    r'\{\{ env\.NOTEBOOKS_SCHEMA \}\}': notebooks_schema,
    r'\{\{ env\.CI_PROJECT_DIR \}\}': script_dir,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/06_configure_aisql_examples.sql', 'w') as f:
    f.write(content)

print("✓ AISQL examples configuration script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ AISQL examples configuration generated${NC}"

# Step 7: Hackathon Database
echo -e "${YELLOW}[9/10] Creating hackathon database configuration script...${NC}"

python3 << 'PYTHON_SCRIPT'
import os
import re

script_dir = os.environ.get('SCRIPT_DIR')
sql_dir = os.environ.get('SQL_DIR')
deploy_dir = os.environ.get('DEPLOY_DIR')
database = os.environ.get('SNOWFLAKE_DATABASE')
warehouse = os.environ.get('SNOWFLAKE_WAREHOUSE')
role = os.environ.get('SNOWFLAKE_ROLE')
schema = os.environ.get('SNOWFLAKE_SCHEMA')
doc_ai_schema = os.environ.get('DOCUMENT_AI_SCHEMA')
hackathon_database = os.environ.get('HACKATHON_DATABASE')

# Read configure_hackathon_database.template.sql
with open(f'{sql_dir}/configure_hackathon_database.template.sql', 'r') as f:
    content = f.read()

# Replace template variables
replacements = {
    r'\{\{ env\.EVENT_DATABASE \}\}': database,
    r'\{\{ env\.EVENT_WAREHOUSE \}\}': warehouse,
    r'\{\{ env\.EVENT_ATTENDEE_ROLE \}\}': role,
    r'\{\{ env\.EVENT_SCHEMA \}\}': schema,
    r'\{\{ env\.DOCUMENT_AI_SCHEMA \}\}': doc_ai_schema,
    r'\{\{ env\.HACKATHON_DATABASE \}\}': hackathon_database,
}

for pattern, replacement in replacements.items():
    content = re.sub(pattern, replacement, content)

# Write output
with open(f'{deploy_dir}/07_configure_hackathon_database.sql', 'w') as f:
    f.write(content)

print("✓ Hackathon database configuration script generated")
PYTHON_SCRIPT

echo -e "${GREEN}✓ Hackathon database configuration generated${NC}"

# Create master deployment script
echo -e "${YELLOW}[10/10] Creating master deployment script...${NC}"

cat > "$DEPLOY_DIR/deploy_all.sql" << EOF
-- FSI Cortex Assistant - Master Deployment Script
-- Generated: $(date)
-- 
-- Deploy using SnowCLI:
-- snow sql -f deployment/deploy_all.sql -c <your_connection>
--
-- Or in Snowflake UI:
-- Copy and paste this entire file into a SQL worksheet

-- ══════════════════════════════════════════════════════════════
-- STEP 1: CONFIGURE ACCOUNT
-- ══════════════════════════════════════════════════════════════

!source deployment/01_configure_account.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 2: DEPLOY DATA FOUNDATION (Creates 20+ tables, 5 search services)
-- ══════════════════════════════════════════════════════════════

!source deployment/02_data_foundation.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 3: DEPLOY CORTEX ANALYST (Creates 2 semantic views + One Ticker agent)
-- ══════════════════════════════════════════════════════════════

!source deployment/03_deploy_cortex_analyst.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 4: DEPLOY STREAMLIT APP (StockOne with REST API)
-- ══════════════════════════════════════════════════════════════

!source deployment/04_deploy_streamlit.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 5: DEPLOY NOTEBOOKS (4 Jupyter notebooks)
-- ══════════════════════════════════════════════════════════════

!source deployment/05_deploy_notebooks.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 6: CONFIGURE AISQL EXAMPLES (AISQL notebooks & data)
-- ══════════════════════════════════════════════════════════════

!source deployment/06_configure_aisql_examples.sql

-- ══════════════════════════════════════════════════════════════
-- STEP 7: CONFIGURE HACKATHON DATABASE (Clone tables/views/stages)
-- ══════════════════════════════════════════════════════════════

!source deployment/07_configure_hackathon_database.sql

-- ══════════════════════════════════════════════════════════════
-- DEPLOYMENT COMPLETE!
-- ══════════════════════════════════════════════════════════════

SELECT 'FSI Cortex Assistant deployment completed successfully!' AS status;

SELECT 'Next Steps:' AS message
UNION ALL SELECT '1. Navigate to AI & ML Studio → Streamlit → STOCKONE_AGENT'
UNION ALL SELECT '2. Navigate to AI & ML Studio → Notebooks → Run notebooks'
UNION ALL SELECT '3. Navigate to AI & ML Studio → Snowflake Intelligence → One Ticker agent'
UNION ALL SELECT '4. Navigate to AI & ML Studio → Cortex Search → Try search services'
UNION ALL SELECT '5. Navigate to AI & ML Studio → Cortex Analyst → Query semantic views';

EOF

echo -e "${GREEN}✓ Master deployment script generated${NC}"
echo ""

# Create README for deployment
cat > "$DEPLOY_DIR/README.md" << 'EOF'
# Deployment Scripts

This directory contains generated SQL scripts for deploying the FSI Cortex Assistant using SnowCLI.

## Prerequisites

1. **Install SnowCLI**:
   ```bash
   pip install snowflake-cli-labs
   ```

2. **Configure SnowCLI Connection**:
   ```bash
   snow connection add
   ```
   
   Enter your Snowflake account details:
   - Account: `<your_account>.snowflakecomputing.com`
   - User: `<your_username>`
   - Password: `<your_password>`
   - Role: `ACCOUNTADMIN`
   - Warehouse: `COMPUTE_WH` (or any existing warehouse)

3. **Test Connection**:
   ```bash
   snow connection test
   ```

## Deployment Options

### Option 1: Deploy Everything at Once

```bash
snow sql -f deployment/deploy_all.sql -c <your_connection_name>
```

This will:
1. Configure account (database, schemas, roles)
2. Deploy data foundation (20+ tables, 5 search services)
3. Deploy Cortex Analyst (2 semantic views, agent)
4. Deploy Streamlit app (StockOne)
5. Deploy notebooks (4 Jupyter notebooks)
6. Configure AISQL examples (AISQL notebooks & data)
7. Configure hackathon database (clone tables/views/stages)

**Estimated Time**: 15-20 minutes

### Option 2: Deploy Step-by-Step

```bash
# Step 1: Configure account
snow sql -f deployment/01_configure_account.sql -c <connection>

# Step 2: Deploy data foundation
snow sql -f deployment/02_data_foundation.sql -c <connection>

# Step 3: Deploy Cortex Analyst
snow sql -f deployment/03_deploy_cortex_analyst.sql -c <connection>

# Step 4: Deploy Streamlit
snow sql -f deployment/04_deploy_streamlit.sql -c <connection>

# Step 5: Deploy notebooks
snow sql -f deployment/05_deploy_notebooks.sql -c <connection>

# Step 6: Configure AISQL examples
snow sql -f deployment/06_configure_aisql_examples.sql -c <connection>

# Step 7: Configure hackathon database
snow sql -f deployment/07_configure_hackathon_database.sql -c <connection>
```

### Option 3: Deploy via Snowflake UI

1. Open Snowflake UI → SQL Worksheets
2. Copy/paste each script in order:
   - 01_configure_account.sql
   - 02_data_foundation.sql
   - 03_deploy_cortex_analyst.sql
   - 04_deploy_streamlit.sql
   - 05_deploy_notebooks.sql
   - 06_configure_aisql_examples.sql
   - 07_configure_hackathon_database.sql

## Verification

After deployment, verify objects created:

```sql
-- Check database and schemas
USE DATABASE ACCELERATE_AI_IN_FSI;
SHOW SCHEMAS;

-- Check tables (should see 20+)
SHOW TABLES IN ACCELERATE_AI_IN_FSI.DEFAULT_SCHEMA;

-- Check search services (should see 5)
SHOW CORTEX SEARCH SERVICES IN ACCELERATE_AI_IN_FSI.DEFAULT_SCHEMA;

-- Check semantic views (should see 2)
SHOW SEMANTIC VIEWS IN ACCELERATE_AI_IN_FSI.CORTEX_ANALYST;

-- Check Streamlit app
SHOW STREAMLITS IN ACCELERATE_AI_IN_FSI.STREAMLIT;

-- Check notebooks
SHOW NOTEBOOKS IN ACCELERATE_AI_IN_FSI.NOTEBOOKS;

-- Check AISQL examples database
USE DATABASE AISQL_EXAMPLES;
SHOW SCHEMAS;
SHOW TABLES IN AISQL_EXAMPLES.DEFAULT_SCHEMA;
SHOW NOTEBOOKS IN AISQL_EXAMPLES.NOTEBOOKS;

-- Check hackathon database
USE DATABASE HACKATHON_DATASETS;
SHOW SCHEMAS;
SHOW TABLES IN HACKATHON_DATASETS.DEFAULT_SCHEMA;
SHOW TABLES IN HACKATHON_DATASETS.DOCUMENT_AI;
```

## Customization

Edit `00_config.sql` to change default values:

```sql
SET DATABASE_NAME = 'MY_CUSTOM_DATABASE';
SET WAREHOUSE_NAME = 'MY_WAREHOUSE';
SET ATTENDEE_ROLE_NAME = 'MY_ROLE';
```

Then re-run the parent `deploy.sh` script to regenerate all files.

## Troubleshooting

### Issue: "Database already exists"

If redeploying, drop the database first:

```sql
USE ROLE ACCOUNTADMIN;
DROP DATABASE IF EXISTS ACCELERATE_AI_IN_FSI CASCADE;
```

### Issue: "Insufficient privileges"

Ensure you're using a role with ACCOUNTADMIN or sufficient privileges:

```sql
USE ROLE ACCOUNTADMIN;
-- Re-run deployment
```

### Issue: "File not found" during PUT

Ensure you're running from the repository root directory:

```bash
cd /path/to/fsi-cortex-assistant
./deploy.sh
```

## Support

For issues or questions:
- Check DEPLOYMENT_README.md in repository root
- Review quickstart.md for detailed guide
- Open GitHub issue on repository
EOF

echo -e "${GREEN}✓ Deployment README generated${NC}"
echo ""

echo -e "${BLUE}╔══════════════════════════════════════════════════════════════╗${NC}"
echo -e "${BLUE}║  Deployment Scripts Generated Successfully!                  ║${NC}"
echo -e "${BLUE}╚══════════════════════════════════════════════════════════════╝${NC}"
echo ""
echo -e "${GREEN}Generated files in ${DEPLOY_DIR}:${NC}"
echo "  - 00_config.sql"
echo "  - 01_configure_account.sql"
echo "  - 02_data_foundation.sql"
echo "  - 03_deploy_cortex_analyst.sql"
echo "  - 04_deploy_streamlit.sql"
echo "  - 05_deploy_notebooks.sql"
echo "  - 06_configure_aisql_examples.sql"
echo "  - 07_configure_hackathon_database.sql"
echo "  - deploy_all.sql (master script)"
echo "  - README.md (deployment guide)"
echo ""
echo -e "${YELLOW}Next Steps:${NC}"
echo "1. Configure SnowCLI: ${BLUE}snow connection add${NC}"
echo "2. Test connection: ${BLUE}snow connection test${NC}"
echo "3. Deploy: ${BLUE}snow sql -f deployment/deploy_all.sql -c <connection>${NC}"
echo ""
echo -e "${GREEN}Or deploy via Snowflake UI by copying the SQL files${NC}"
echo ""


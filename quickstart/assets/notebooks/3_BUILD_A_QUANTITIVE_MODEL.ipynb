{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2dgnbqp5anw5ovpri7bj",
   "authorId": "7518827724888",
   "authorName": "USER",
   "authorEmail": "",
   "sessionId": "aef52695-e0ff-49ce-855b-52c93b07b132",
   "lastEditTime": 1761652100441
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f89fa7-3d1d-4c88-be95-888f326dfcb1",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": "# Building A Quantitive Model\nSnowflake has been the trusted data platform for our financial services customers, and we are excited to show case all our latest AI functionality to further supercharge your workflow. In this notebook, we will walk through:\n\n1. Leverage Snowflake marketplace to access market data\n2. Leverage Snowflake Cortex AISQL to process the unstructured data\n3. Leverage Snowflake Data Science Agent to accelerate ML model building"
  },
  {
   "cell_type": "code",
   "id": "c16761b4-a16f-4602-9c8b-8a0a1d0dd028",
   "metadata": {
    "language": "sql",
    "name": "transcripts_from_marketplace"
   },
   "outputs": [],
   "source": "select * from ACCELERATE_AI_IN_FSI.DEFAULT_SCHEMA.UNIQUE_TRANSCRIPTS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7eab4c1-3a88-4405-a5dc-a3f12b74573e",
   "metadata": {
    "language": "sql",
    "name": "sentiments",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DEFAULT_SCHEMA.ai_transcripts_analysts_sentiments AS\n\nSELECT PRIMARY_TICKER, EVENT_TIMESTAMP, EVENT_TYPE, CREATED_AT, SENTIMENT_SCORE, EMOJI, UNIQUE_ANALYST_COUNT, SENTIMENT_REASON FROM \n\n(\n\n{% raw %}\nSELECT * FROM (\nWITH \nparsed_transcripts AS (\n    SELECT\n        primary_ticker,\n        event_timestamp,\n        event_type,\n        created_at,\n        PARSE_JSON(transcript) AS transcript_json\n    FROM unique_transcripts\n),\nspeaker_lookup AS (\n    SELECT\n        primary_ticker,\n        event_timestamp,\n        OBJECT_AGG(\n            speaker_data.value:speaker::STRING,\n            OBJECT_CONSTRUCT(\n                'name', speaker_data.value:speaker_data.name::STRING,\n                'role', speaker_data.value:speaker_data.role::STRING\n            )\n        ) AS speakers\n    FROM parsed_transcripts,\n    LATERAL FLATTEN(input => transcript_json:speaker_mapping) speaker_data\n    GROUP BY primary_ticker, event_timestamp\n),\nformatted_transcripts AS (\n    SELECT\n        p.primary_ticker,\n        p.event_timestamp,\n        p.event_type,\n        p.created_at,\n        LISTAGG(\n            s.speakers[parsed_entry.value:speaker::STRING]:name::STRING ||\n            CASE \n                WHEN s.speakers[parsed_entry.value:speaker::STRING]:role::STRING IS NOT NULL\n                THEN ' (' || s.speakers[parsed_entry.value:speaker::STRING]:role::STRING || '): '\n                ELSE ': '\n            END ||\n            parsed_entry.value:text::STRING,\n            '\\n\\n'\n        ) WITHIN GROUP (ORDER BY parsed_entry.index) AS transcript_text\n    FROM parsed_transcripts p\n    JOIN speaker_lookup s ON p.primary_ticker = s.primary_ticker AND p.event_timestamp = s.event_timestamp\n    CROSS JOIN LATERAL FLATTEN(input => p.transcript_json:parsed_transcript) parsed_entry\n    GROUP BY p.primary_ticker, p.event_timestamp, p.event_type, p.created_at\n),\nai_analysis AS (\n    SELECT\n        primary_ticker,\n        event_timestamp,\n        event_type,\n        created_at,\n        ai_complete(\n            'claude-4-sonnet',\n            'Analyze analyst sentiment in this earnings call. Speakers with \"(Analyst)\" in their label are analysts. ' ||\n            'Rate sentiment 1-10 based on analyst questions and tone. ' ||\n            'Return ONLY valid JSON with no extra text: {\"score\": <1-10>, \"emoji\": \"<emoji>\", \"reason\": \"<brief text>\", \"analyst_count\": <number>}' ||\n            '\\n\\nTranscript:\\n\\n' ||\n            transcript_text\n        ) AS ai_response\n    FROM formatted_transcripts\n),\ncleaned_responses AS (\n    SELECT\n        primary_ticker,\n        event_timestamp,\n        event_type,\n        created_at,\n        ai_response,\n        CASE \n            WHEN ai_response LIKE '%```json%```%' THEN\n                REGEXP_SUBSTR(ai_response, '\\{[^}]*\\}', 1, 1, 'e')\n            WHEN ai_response LIKE '%```%{%}%```%' THEN\n                REGEXP_SUBSTR(ai_response, '\\{[^}]*\\}', 1, 1, 'e')\n            WHEN ai_response LIKE '{%' THEN\n                REGEXP_SUBSTR(ai_response, '\\{[^}]*\\}', 1, 1, 'e')\n            ELSE\n                REGEXP_SUBSTR(ai_response, '\\{[^}]*\\}', 1, 1, 'e')\n        END AS extracted_json\n    FROM ai_analysis\n)\nSELECT\n    primary_ticker,\n    event_timestamp,\n    event_type,\n    created_at,\n    ai_response,\n    extracted_json,\n    (TRY_PARSE_JSON(extracted_json):emoji)::TEXT AS emoji,\n    (TRY_PARSE_JSON(extracted_json):score)::INT AS sentiment_score,\n    (TRY_PARSE_JSON(extracted_json):analyst_count)::INT AS unique_analyst_count,\n    (TRY_PARSE_JSON(extracted_json):reason)::STRING AS sentiment_reason\nFROM cleaned_responses\nORDER BY primary_ticker, event_timestamp\n));\n{% endraw %}\n\nSELECT * FROM ai_transcripts_analysts_sentiments",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00b5076d-2c0b-4927-96bb-264d12ea6e92",
   "metadata": {
    "language": "python",
    "name": "live_stock_data",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\nCORE_TICKERS = ['SNOW', 'NRNT', 'ICBG', 'QRYQ', 'DFLX', 'STRM', 'VLTA', 'CTLG']\n# --- Title and Introduction ---\nst.title(\"❄️ Live FSI Data from Snowflake for Financial Services\")\nst.markdown(\n    \"This app runs directly in Snowflake and uses the data from the **Snowflake Marketplace**.\"\n)\n\n# --- Get Active Session and Query Data ---\n# Get the current session object provided by the Streamlit in Snowflake environment\nsession = get_active_session()\n\n# This query now calculates the offset needed to make the latest date appear as '2025-11-01'\nquery = \"\"\"\nWITH DateOffset AS (\n    SELECT\n        DATEDIFF(\n            day,\n            MAX(TO_TIMESTAMP_NTZ(DATE, 9)::DATE),\n            '2025-11-01'::DATE\n        ) AS days_to_add\n    FROM DEFAULT_SCHEMA.FSI_DATA\n)\nSELECT\n    TICKER,\n    ASSET_CLASS,\n    PRIMARY_EXCHANGE_NAME,\n    DATEADD(\n        day,\n        (SELECT days_to_add FROM DateOffset),\n        TO_TIMESTAMP_NTZ(DATE, 9)::DATE\n    ) as DATE,\n    PRICE,\n    RETURN,\n    IS_SPLIT,\n    Y\nFROM DEFAULT_SCHEMA.FSI_DATA\nWHERE TICKER IN ('SNOW', 'NRNT', 'ICBG', 'QRYQ', 'DFLX', 'STRM', 'VLTA', 'CTLG')\nORDER BY DATE DESC;\n\"\"\"\n\n# Execute the query using the Snowpark session and convert to a Pandas DataFrame\ndata_frame = session.sql(query).to_pandas()\n\n\n# --- Display the \"Nice Looking\" Dataframe ---\nst.header(\"FSI Data Explorer\")\nst.dataframe(\n    data_frame,\n    use_container_width=True,\n    hide_index=True,\n    column_config={\n        \"TICKER\": st.column_config.TextColumn(\n            \"Ticker Symbol\",\n            help=\"The stock ticker symbol.\",\n            width=\"small\"\n        ),\n        \"ASSET_CLASS\": st.column_config.TextColumn(\n            \"Asset Class\",\n            width=\"small\"\n        ),\n        \"PRIMARY_EXCHANGE_NAME\": \"Primary Exchange\",\n        \"DATE\": st.column_config.DateColumn(\n            \"Date\",\n            format=\"DD MMM, YYYY\" # Format for readability\n        ),\n        \"PRICE\": st.column_config.NumberColumn(\n            \"Price (USD)\",\n            help=\"Closing price in US Dollars.\",\n            format=\"$%.2f\"\n        ),\n        \"RETURN\": st.column_config.NumberColumn(\n            \"Daily Return\",\n            help=\"The daily percentage return.\",\n            format=\"%.2f%%\"\n        ),\n        \"IS_SPLIT\": st.column_config.CheckboxColumn(\n            \"Stock Split?\",\n            help=\"Indicates if a stock split occurred.\",\n            width=\"small\"\n        ),\n        \"Y\": st.column_config.ProgressColumn(\n            \"Target Probability (Y)\",\n            help=\"Model prediction or target variable.\",\n            format=\"%.3f\",\n            min_value=0,\n            max_value=1,\n        ),\n    }\n)\n\n# --- NEW SECTION: Daily Price Charts ---\nst.header(\"Daily Price Charts - Core 8 Tickers\")\n\n# Use the 8 core tickers in the specific order\ntickers_to_plot = CORE_TICKERS\n\n# Only create the columns and charts if we found any tickers\nif tickers_to_plot:\n    # First row - First 4 tickers\n    st.subheader(\"Row 1: Market Leaders & Competitors\")\n    cols_row1 = st.columns(4)\n    \n    for i, ticker in enumerate(tickers_to_plot[:4]):\n        with cols_row1[i]:\n            st.subheader(ticker)\n            \n            # Filter the dataframe for the current ticker\n            ticker_data = data_frame[data_frame[\"TICKER\"] == ticker]\n            \n            if len(ticker_data) > 0:\n                # Calculate the min and max price for *this specific ticker* to set a dynamic y-axis\n                min_price = ticker_data['PRICE'].min()\n                max_price = ticker_data['PRICE'].max()\n                \n                # Use Altair to create a chart with a custom y-axis and Snowflake blue line\n                chart = alt.Chart(ticker_data).mark_line(color='#00B2EE').encode(\n                    x=alt.X('DATE', title='Date'),\n                    y=alt.Y('PRICE', title='Price', scale=alt.Scale(domain=[min_price - (min_price*0.05), max_price + (max_price*0.05)])),\n                    tooltip=['DATE', 'PRICE']\n                ).interactive()\n                \n                # Display the Altair chart\n                st.altair_chart(chart, use_container_width=True)\n            else:\n                st.warning(f\"No data available for {ticker}\")\n    \n    # Second row - Last 4 tickers\n    st.subheader(\"Row 2: Platform Providers\")\n    cols_row2 = st.columns(4)\n    \n    for i, ticker in enumerate(tickers_to_plot[4:8]):\n        with cols_row2[i]:\n            st.subheader(ticker)\n            \n            # Filter the dataframe for the current ticker\n            ticker_data = data_frame[data_frame[\"TICKER\"] == ticker]\n            \n            if len(ticker_data) > 0:\n                # Calculate the min and max price for *this specific ticker* to set a dynamic y-axis\n                min_price = ticker_data['PRICE'].min()\n                max_price = ticker_data['PRICE'].max()\n                \n                # Use Altair to create a chart with a custom y-axis and Snowflake blue line\n                chart = alt.Chart(ticker_data).mark_line(color='#00B2EE').encode(\n                    x=alt.X('DATE', title='Date'),\n                    y=alt.Y('PRICE', title='Price', scale=alt.Scale(domain=[min_price - (min_price*0.05), max_price + (max_price*0.05)])),\n                    tooltip=['DATE', 'PRICE']\n                ).interactive()\n                \n                # Display the Altair chart\n                st.altair_chart(chart, use_container_width=True)\n            else:\n                st.warning(f\"No data available for {ticker}\")\nelse:\n    st.info(\"No tickers found in this data sample to plot.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c2e10a5-2105-4294-a7cb-bc3ef331ed7c",
   "metadata": {
    "collapsed": false,
    "name": "Data_Science_Agent"
   },
   "source": "# Data Science Agent: Financial ML Model\nSoon, you aill have the ability to use a **Data Science Agent** which will plan and create models for you.  the Python Scripts below have ben Generated by the Agent which is currently in Private Preview.  They were created using the prompts described in the markdown above the code."
  },
  {
   "cell_type": "markdown",
   "id": "3afbade0-81b6-4490-8f0c-3289e756928f",
   "metadata": {
    "name": "prompt1",
    "collapsed": false
   },
   "source": "# Prompt 1 Feature Engineering - \n\nHelp me construct features with returns, eg. the last 1 day return using close price, return from **t-4** to **t-1**, return from **t-9** to **t-5**, return from **t-20** to **t-11**, and return from **t-62** to **t-21**. I also want to construct my predictive variable, which should be the future return from **t+2** to **t+6**. Please also help take the log across all return variables, including the previously constructed feature and this predictive variable. Please keep as panel data where ticker is a column."
  },
  {
   "cell_type": "code",
   "id": "563ee63b-b13a-494d-b6ce-6a95b84f47a3",
   "metadata": {
    "language": "python",
    "name": "Feature_Engineering",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Generated by Snowflake Copilot\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nimport numpy as np\n\n# Get session\nsession = get_active_session()\n\n# Get the data with necessary columns\ndata = session.table('DEFAULT_SCHEMA.FSI_DATA') \\\n    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n    .to_pandas()\n\n# Calculate future return (t+2 to t+5)\ndata['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n\n# Apply log transformation to all return columns\nreturn_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\nfor col in return_cols:\n    data[f'LOG_{col}'] = np.log(1 + data[col])\n\n# Select final columns\nfinal_columns = ['TICKER', 'DATE'] + [f'LOG_{col}' for col in return_cols]\nfinal_data = data[final_columns]\n\n# Sort by ticker and date\nfinal_data = final_data.sort_values(['TICKER', 'DATE'])\n\nprint(f\"Final dataset shape: {final_data.shape}\")\nprint(\"\\nFirst few rows of the processed data:\")\nprint(final_data.head())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16f71e32-d13e-4d36-ba29-e7206797482f",
   "metadata": {
    "name": "prompt_2",
    "collapsed": false
   },
   "source": "# Prompt 2 - Train a Predictive Model\n\nWith those features, I want to train a predictive **lightGBM** model with **L2 metric**. Please do a walk forward training on a quarterly basis. For each test quarter:\n- **Train** on all quarters \\< (Q-2)\n- **Validate** on (Q-2, Q-1)\n- **Test** on **Q**\n- Enforce strict cutoffs so rows needing returns beyond the split end are dropped (no look-ahead).\n\n\n### The Result"
  },
  {
   "cell_type": "code",
   "id": "599b75a5-ccc5-43ed-a014-38ca9f03b5e9",
   "metadata": {
    "language": "python",
    "name": "Training_ML_Model",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Generated by Snowflake Copilot\nimport pandas as pd\nimport numpy as np\nfrom snowflake.snowpark.context import get_active_session\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n# Get session and data\nsession = get_active_session()\ndata = session.table('DEFAULT_SCHEMA.FSI_DATA') \\\n    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n    .to_pandas()\n\n# Calculate future return and log transform returns\ndata['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n\n# Log transform all return columns\nreturn_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\nfor col in return_cols:\n    data[f'LOG_{col}'] = np.log(1 + data[col])\n\n# Create quarter information\ndata['DATE'] = pd.to_datetime(data['DATE'])\ndata['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\n\n# Prepare features and target\nfeature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\ntarget_col = 'LOG_FUTURE_RETURN_2_5'\n\n# Get all quarters from 2020Q1 onwards\nall_quarters = sorted([q for q in data['YEAR_QUARTER'].unique() if q >= pd.Period('2020Q1', freq='Q')])\n\n# Initialize results storage\nresults = []\n\n# Walk-forward testing with improved model parameters\nfor test_quarter in all_quarters:\n    # Define validation quarters (Q-2, Q-1)\n    val_quarters = [test_quarter - 2, test_quarter - 1]\n    \n    # Split the data\n    train_data = data[data['YEAR_QUARTER'] < val_quarters[0]]\n    val_data = data[data['YEAR_QUARTER'].isin(val_quarters)]\n    test_data = data[data['YEAR_QUARTER'] == test_quarter]\n    \n    # Create datasets\n    X_train = train_data[feature_cols]\n    y_train = train_data[target_col]\n    X_val = val_data[feature_cols]\n    y_val = val_data[target_col]\n    X_test = test_data[feature_cols]\n    y_test = test_data[target_col]\n    \n    # Train model with improved parameters\n    model = lgb.LGBMRegressor(\n        objective='regression',\n        metric='l2',\n        n_estimators=200,\n        learning_rate=0.05,\n        num_leaves=31,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    \n    # Train with validation set\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n    \n    # Make predictions\n    test_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, test_pred)\n    r2 = 1 - np.sum((y_test - test_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n    \n    # Store results\n    results.append({\n        'quarter': test_quarter,\n        'train_size': len(X_train),\n        'val_size': len(X_val),\n        'test_size': len(X_test),\n        'mse': mse,\n        'r2': r2\n    })\n\n# Print results\nprint(\"Walk-forward testing results:\")\nfor r in results:\n    print(f\"\\nQuarter: {r['quarter']}\")\n    print(f\"Train size: {r['train_size']}, Val size: {r['val_size']}, Test size: {r['test_size']}\")\n    print(f\"MSE: {r['mse']:.6f}\")\n    print(f\"R2: {r['r2']:.6f}\")\n\n# Calculate and print average metrics\navg_mse = np.mean([r['mse'] for r in results])\navg_r2 = np.mean([r['r2'] for r in results])\nprint(f\"\\nAverage MSE across all quarters: {avg_mse:.6f}\")\nprint(f\"Average R2 across all quarters: {avg_r2:.6f}\")\n\n# Save predictions for the most recent quarter\nlast_quarter = all_quarters[-1]\nlast_quarter_data = data[data['YEAR_QUARTER'] == last_quarter].copy()\nX_last = last_quarter_data[feature_cols]\nlast_quarter_data['predicted_return'] = model.predict(X_last)\nfinal_predictions = last_quarter_data[['TICKER', 'DATE', target_col, 'predicted_return']]\nprint(\"\\nSample of final predictions for the last quarter:\")\nprint(final_predictions.head())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef9a89cc-7497-476f-b53c-94629bb41839",
   "metadata": {
    "name": "prompt_3",
    "collapsed": false
   },
   "source": "## Prompt 3 - Backtest the model\n\nLet's test if the strategy can work, starting 2021, for each portfolio construction, generate forecasts on **Tuesdays** in the test quarter.\n- At **Wednesday close**, go **long top-5** and **short bottom-5** by predicted return (equal weight).\n- Hold through **Thu → next Wed** (exactly the `t+2..t+6` window used for training).\n- **Transaction cost**: 3.0 bps one-way applied via weekly turnover.\nShow Strategy metrics:\n- Information Ratio (before/after costs)\n- Max drawdown\n- At the end plot the equity curve on before & after cost."
  },
  {
   "cell_type": "code",
   "id": "09b9dcd9-7bc9-4705-9aea-8d5e64237db0",
   "metadata": {
    "language": "python",
    "name": "Backtesting",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Generated by Snowflake Copilot\nimport pandas as pd\nimport numpy as np\nfrom snowflake.snowpark.context import get_active_session\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Get session and data\nsession = get_active_session()\ndata = session.table('DEFAULT_SCHEMA.FSI_DATA') \\\n    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n    .to_pandas()\n\n# Calculate future return and log transform\ndata['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n\nreturn_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\nfor col in return_cols:\n    data[f'LOG_{col}'] = np.log(1 + data[col])\n\n# Create quarter and day information\ndata['DATE'] = pd.to_datetime(data['DATE'])\ndata['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\ndata['DAY_OF_WEEK'] = data['DATE'].dt.day_name()\n\n# Prepare features and target\nfeature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\ntarget_col = 'LOG_FUTURE_RETURN_2_5'\n\n# Get quarters from 2021Q1 onwards for strategy testing\nstrategy_quarters = sorted([q for q in data['YEAR_QUARTER'].unique() if q >= pd.Period('2021Q1', freq='Q')])\n\n# Initialize storage for all predictions\nall_predictions = []\n\n# Walk-forward testing starting from 2021Q1\nfor test_quarter in strategy_quarters:\n    # Define validation quarters (Q-2, Q-1)\n    val_quarters = [test_quarter - 2, test_quarter - 1]\n    \n    # Split the data - train on all quarters < (Q-2)\n    train_data = data[data['YEAR_QUARTER'] < val_quarters[0]]\n    val_data = data[data['YEAR_QUARTER'].isin(val_quarters)]\n    test_data = data[data['YEAR_QUARTER'] == test_quarter]\n    \n    # Skip if insufficient training data\n    if len(train_data) < 1000:\n        continue\n    \n    # Prepare training data\n    X_train = train_data[feature_cols].dropna()\n    y_train = train_data[target_col].loc[X_train.index]\n    X_val = val_data[feature_cols].dropna()\n    y_val = val_data[target_col].loc[X_val.index]\n    \n    # Train LightGBM model\n    model = lgb.LGBMRegressor(\n        objective='regression',\n        metric='l2',\n        n_estimators=200,\n        learning_rate=0.05,\n        num_leaves=31,\n        min_child_samples=20,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        verbose=-1\n    )\n    \n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n    \n    # Get Tuesdays in the test quarter for prediction\n    test_tuesdays = test_data[test_data['DAY_OF_WEEK'] == 'Tuesday'].copy()\n    \n    if len(test_tuesdays) > 0:\n        # Generate predictions for Tuesdays\n        X_test_tuesday = test_tuesdays[feature_cols].dropna()\n        if len(X_test_tuesday) > 0:\n            predictions = model.predict(X_test_tuesday)\n            test_tuesdays_clean = test_tuesdays.loc[X_test_tuesday.index].copy()\n            test_tuesdays_clean['predicted_return'] = predictions\n            test_tuesdays_clean['test_quarter'] = test_quarter\n            all_predictions.append(test_tuesdays_clean)\n\n# Combine all predictions\npredictions_df = pd.concat(all_predictions, ignore_index=True)\n\n# Strategy implementation\ndef calculate_strategy_returns(predictions_df):\n    strategy_results = []\n    \n    # Group by date (Tuesday predictions)\n    for date, group in predictions_df.groupby('DATE'):\n        # Rank by predicted return\n        group_sorted = group.sort_values('predicted_return', ascending=False)\n        \n        # Select top 5 (long) and bottom 5 (short)\n        if len(group_sorted) >= 10:\n            long_positions = group_sorted.head(5).copy()\n            short_positions = group_sorted.tail(5).copy()\n            \n            # Calculate portfolio return (equal weight)\n            long_return = long_positions['LOG_FUTURE_RETURN_2_5'].mean()\n            short_return = short_positions['LOG_FUTURE_RETURN_2_5'].mean()\n            \n            # Long-short strategy return\n            strategy_return = long_return - short_return\n            \n            # Store results\n            strategy_results.append({\n                'date': date,\n                'long_return': long_return,\n                'short_return': short_return,\n                'strategy_return': strategy_return,\n                'num_stocks': len(group_sorted),\n                'long_tickers': list(long_positions['TICKER']),\n                'short_tickers': list(short_positions['TICKER'])\n            })\n    \n    return pd.DataFrame(strategy_results)\n\n# Calculate strategy returns\nstrategy_df = calculate_strategy_returns(predictions_df)\n\n# Calculate transaction costs based on turnover\ndef calculate_turnover_and_costs(strategy_df):\n    strategy_df = strategy_df.copy()\n    strategy_df['turnover'] = 0.0\n    strategy_df['transaction_cost'] = 0.0\n    \n    prev_long = set()\n    prev_short = set()\n    \n    for i, row in strategy_df.iterrows():\n        current_long = set(row['long_tickers'])\n        current_short = set(row['short_tickers'])\n        \n        if i > 0:\n            # Calculate turnover as fraction of positions that changed\n            long_changes = len(current_long.symmetric_difference(prev_long))\n            short_changes = len(current_short.symmetric_difference(prev_short))\n            total_changes = long_changes + short_changes\n            total_positions = 10  # 5 long + 5 short\n            \n            turnover = total_changes / total_positions\n            # Transaction cost: 3.0 bps one-way on turnover\n            transaction_cost = turnover * 0.0003  # 3.0 bps = 0.0003\n            \n            strategy_df.loc[i, 'turnover'] = turnover\n            strategy_df.loc[i, 'transaction_cost'] = transaction_cost\n        \n        prev_long = current_long\n        prev_short = current_short\n    \n    return strategy_df\n\n# Calculate costs\nstrategy_df = calculate_turnover_and_costs(strategy_df)\n\n# Calculate net returns after transaction costs\nstrategy_df['net_return'] = strategy_df['strategy_return'] - strategy_df['transaction_cost']\n\n# Calculate cumulative returns and strategy metrics\nstrategy_df['cumulative_gross'] = (1 + strategy_df['strategy_return']).cumprod()\nstrategy_df['cumulative_net'] = (1 + strategy_df['net_return']).cumprod()\n\n# Calculate Information Ratio (annualized)\nweeks_per_year = 52\ngross_annual_return = strategy_df['strategy_return'].mean() * weeks_per_year\nnet_annual_return = strategy_df['net_return'].mean() * weeks_per_year\ngross_annual_vol = strategy_df['strategy_return'].std() * np.sqrt(weeks_per_year)\nnet_annual_vol = strategy_df['net_return'].std() * np.sqrt(weeks_per_year)\n\nir_gross = gross_annual_return / gross_annual_vol\nir_net = net_annual_return / net_annual_vol\n\n# Calculate Maximum Drawdown\ndef calculate_max_drawdown(cumulative_returns):\n    peak = cumulative_returns.expanding().max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\nmax_dd_gross = calculate_max_drawdown(strategy_df['cumulative_gross'])\nmax_dd_net = calculate_max_drawdown(strategy_df['cumulative_net'])\n\n# Print strategy metrics\nprint(\"=== STRATEGY PERFORMANCE METRICS ===\")\nprint(f\"\\nGross Performance (Before Costs):\")\nprint(f\"  Annual Return: {gross_annual_return:.2%}\")\nprint(f\"  Annual Volatility: {gross_annual_vol:.2%}\")\nprint(f\"  Information Ratio: {ir_gross:.3f}\")\nprint(f\"  Maximum Drawdown: {max_dd_gross:.2%}\")\n\nprint(f\"\\nNet Performance (After Costs):\")\nprint(f\"  Annual Return: {net_annual_return:.2%}\")\nprint(f\"  Annual Volatility: {net_annual_vol:.2%}\")\nprint(f\"  Information Ratio: {ir_net:.3f}\")\nprint(f\"  Maximum Drawdown: {max_dd_net:.2%}\")\n\nprint(f\"\\nTransaction Cost Impact:\")\nprint(f\"  Cost Drag: {(gross_annual_return - net_annual_return):.2%} annually\")\nprint(f\"  Average Weekly Turnover: {strategy_df['turnover'].mean():.1%}\")\n\n# Final performance summary\ntotal_gross_return = strategy_df['cumulative_gross'].iloc[-1] - 1\ntotal_net_return = strategy_df['cumulative_net'].iloc[-1] - 1\nprint(f\"\\nTotal Strategy Performance ({strategy_df['date'].min().strftime('%Y-%m-%d')} to {strategy_df['date'].max().strftime('%Y-%m-%d')}):\")\nprint(f\"  Gross Total Return: {total_gross_return:.2%}\")\nprint(f\"  Net Total Return: {total_net_return:.2%}\")\n\n# Create equity curve plot\nplt.figure(figsize=(12, 8))\nplt.plot(strategy_df['date'], strategy_df['cumulative_gross'], label='Gross Returns (Before Costs)', linewidth=2)\nplt.plot(strategy_df['date'], strategy_df['cumulative_net'], label='Net Returns (After Costs)', linewidth=2)\nplt.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Breakeven')\n\nplt.title('Long-Short Strategy Equity Curve (2021-2025)', fontsize=14, fontweight='bold')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Cumulative Return (Base = 1)', fontsize=12)\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Add performance annotations\nfinal_gross = strategy_df['cumulative_gross'].iloc[-1]\nfinal_net = strategy_df['cumulative_net'].iloc[-1]\nplt.text(0.02, 0.98, f'Final Gross: {final_gross:.3f} (+{(final_gross-1)*100:.1f}%)', \n         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\nplt.text(0.02, 0.90, f'Final Net: {final_net:.3f} (+{(final_net-1)*100:.1f}%)', \n         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n\nplt.show()\n\nprint(\"Strategy implementation completed successfully!\")\nprint(f\"Total trading weeks: {len(strategy_df)}\")\nprint(f\"Strategy period: {strategy_df['date'].min().strftime('%Y-%m-%d')} to {strategy_df['date'].max().strftime('%Y-%m-%d')}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "07710bd3-98cf-46a9-a078-ca209ea22e85",
   "metadata": {
    "name": "prompt4",
    "collapsed": false
   },
   "source": "## Prompt 4 - Register the Model\n\nRegister only the final model in Snowflake Registry with the following options:\n\n- model name \"STOCK_RETURN_PREDICTOR_GBM\"\n- sample input of 100 rows\n- target_platforms=[\"WAREHOUSE\"]\n- options={\n    \"relax_version\": False,\n    \"target_methods\": [\"predict\"],\n    \"method_options\": {\n        \"predict\": {\"case_sensitive\": True}\n    }\n}"
  },
  {
   "cell_type": "code",
   "id": "252dfa42-8603-4ac9-a799-bfeeef78df54",
   "metadata": {
    "language": "python",
    "name": "Reg_Model",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Generated by Snowflake Copilot\nfrom snowflake.snowpark.context import get_active_session\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom snowflake.ml.registry import Registry\n\nsession = get_active_session()\n\ndata = session.table('DEFAULT_SCHEMA.FSI_DATA') \\\n    .select(['TICKER', 'DATE', 'R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21',\n             'RETURN_LEAD_2', 'RETURN_LEAD_3', 'RETURN_LEAD_4', 'RETURN_LEAD_5']) \\\n    .to_pandas()\n\ndata['FUTURE_RETURN_2_5'] = (1 + data['RETURN_LEAD_2']) * (1 + data['RETURN_LEAD_3']) * \\\n                           (1 + data['RETURN_LEAD_4']) * (1 + data['RETURN_LEAD_5']) - 1\n\nreturn_cols = ['R_1', 'R_5_1', 'R_10_5', 'R_21_10', 'R_63_21', 'FUTURE_RETURN_2_5']\nfor col in return_cols:\n    data[f'LOG_{col}'] = np.log(1 + data[col])\n\ndata['DATE'] = pd.to_datetime(data['DATE'])\ndata['YEAR_QUARTER'] = data['DATE'].dt.to_period('Q')\n\nfeature_cols = ['LOG_R_1', 'LOG_R_5_1', 'LOG_R_10_5', 'LOG_R_21_10', 'LOG_R_63_21']\ntarget_col = 'LOG_FUTURE_RETURN_2_5'\n\ntraining_data = data[data['YEAR_QUARTER'] < pd.Period('2025Q4', freq='Q')].copy()\ntraining_data_clean = training_data.dropna(subset=feature_cols + [target_col])\n\nX_train = training_data_clean[feature_cols]\ny_train = training_data_clean[target_col]\n\nfinal_model = lgb.LGBMRegressor(\n    objective='regression',\n    metric='l2',\n    n_estimators=200,\n    learning_rate=0.05,\n    num_leaves=31,\n    min_child_samples=20,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    verbose=-1\n)\n\nfinal_model.fit(X_train, y_train)\n\nsample_input = X_train.head(100)\n\nregistry = Registry(session=session)\n\nmodel_ref = registry.log_model(\n    model=final_model,\n    model_name=\"STOCK_RETURN_PREDICTOR_GBM\",\n    sample_input_data=sample_input,\n    target_platforms=[\"WAREHOUSE\"],\n    options={\n        \"relax_version\": False,\n        \"target_methods\": [\"predict\"],\n        \"method_options\": {\n            \"predict\": {\"case_sensitive\": True}\n        }\n    }\n)\n\nprint(f\"Model registered successfully!\")\nprint(f\"Model name: {model_ref.model_name}\")\nprint(f\"Model version: {model_ref.version_name}\")\nprint(f\"Sample input shape: {sample_input.shape}\")\nprint(f\"Feature columns used: {list(sample_input.columns)}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f51b062f-d4cc-4a75-8db6-6b85caf31c8f",
   "metadata": {
    "name": "Register_Model",
    "collapsed": false
   },
   "source": "The Model Registered can now be used as an Agent Tool for Snowflake Intelligence."
  }
 ]
}